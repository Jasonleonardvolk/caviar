#!/usr/bin/env python3\n\"\"\"\nPrajna PDF-Only Data Ingestion System\n====================================\n\nSpecialized PDF processing and ingestion for Prajna's consciousness.\nFocuses exclusively on PDF documents with advanced text extraction.\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nimport time\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport concurrent.futures\nimport hashlib\nimport re\n\n# Add prajna to path\nsys.path.insert(0, str(Path(__file__).parent / \"prajna\"))\n\n# Import Prajna components\ntry:\n    from prajna.memory.context_builder import build_context\n    from prajna.memory.soliton_interface import SolitonMemoryInterface\n    from prajna.memory.concept_mesh_api import ConceptMeshAPI\n    from prajna.config.prajna_config import PrajnaConfig\n    from prajna.core.prajna_mouth import PrajnaLanguageModel\n    PRAJNA_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"⚠️ Prajna not available: {e}\")\n    PRAJNA_AVAILABLE = False\n\n# PDF processing imports\ntry:\n    import PyPDF2\n    PDF_EXTRACTION_AVAILABLE = True\nexcept ImportError:\n    PDF_EXTRACTION_AVAILABLE = False\n    print(\"⚠️ PyPDF2 not available - install with: pip install PyPDF2\")\n\ntry:\n    import fitz  # PyMuPDF\n    PYMUPDF_AVAILABLE = True\nexcept ImportError:\n    PYMUPDF_AVAILABLE = False\n    print(\"💡 PyMuPDF not available - install with: pip install PyMuPDF for better PDF processing\")\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"prajna.pdf_ingest\")\n\n@dataclass\nclass PDFDocument:\n    \"\"\"Represents a PDF document to be ingested\"\"\"\n    path: Path\n    size: int\n    pages: int\n    title: str\n    author: str\n    subject: str\n    content_hash: str\n    text_content: str = \"\"\n    concepts: List[str] = None\n    \n    def __post_init__(self):\n        if self.concepts is None:\n            self.concepts = []\n\n@dataclass\nclass PDFIngestionStats:\n    \"\"\"Statistics for PDF ingestion process\"\"\"\n    total_pdfs: int = 0\n    processed_pdfs: int = 0\n    failed_pdfs: int = 0\n    total_pages: int = 0\n    total_size: int = 0\n    processed_size: int = 0\n    start_time: float = 0.0\n    \n    @property\n    def progress_percent(self) -> float:\n        return (self.processed_pdfs / max(1, self.total_pdfs)) * 100\n    \n    @property\n    def processing_rate(self) -> float:\n        elapsed = time.time() - self.start_time\n        return self.processed_pdfs / max(1, elapsed)\n\nclass PrajnaPDFIngestor:\n    \"\"\"\n    Specialized PDF ingestion system for Prajna's consciousness\n    \n    Features:\n    - Advanced PDF text extraction\n    - Metadata preservation\n    - Concept extraction from content\n    - Title and author analysis\n    - Academic paper recognition\n    - Research document categorization\n    \"\"\"\n    \n    def __init__(self, data_directory: str):\n        self.data_dir = Path(data_directory)\n        self.stats = PDFIngestionStats()\n        self.processed_hashes = set()\n        self.batch_size = 25  # Smaller batches for PDFs\n        self.max_workers = 3  # Fewer workers for intensive PDF processing\n        \n        # PDF-specific patterns\n        self.academic_patterns = [\n            r'\\babstract\\b', r'\\bintroduction\\b', r'\\bmethodology\\b',\n            r'\\bresults\\b', r'\\bconclusion\\b', r'\\breferences\\b',\n            r'\\bbibliography\\b', r'\\backnowledg\\w+\\b'\n        ]\n        \n        self.concept_patterns = [\n            r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b',  # Title case terms\n            r'\\b\\w+(?:tion|sion|ment|ness|ity|ism)\\b',  # Common suffixes\n            r'\\b(?:machine|artificial|deep|neural|quantum|cognitive)\\s+\\w+\\b'  # Tech terms\n        ]\n        \n        logger.info(f\"📚 Prajna PDF Ingestor initialized for: {self.data_dir}\")\n    \n    async def initialize_prajna(self):\n        \"\"\"Initialize Prajna components for PDF ingestion\"\"\"\n        if not PRAJNA_AVAILABLE:\n            logger.warning(\"⚠️ Prajna not available - using mock mode\")\n            return\n        \n        try:\n            logger.info(\"📚 Initializing Prajna components for PDF processing...\")\n            \n            # Load configuration optimized for PDF processing\n            self.prajna_config = PrajnaConfig()\n            self.prajna_config.max_context_length = 8192  # Larger context for PDFs\n            \n            # Initialize memory systems\n            self.soliton_memory = SolitonMemoryInterface()\n            await self.soliton_memory.initialize()\n            \n            self.concept_mesh = ConceptMeshAPI()\n            await self.concept_mesh.initialize()\n            \n            # Initialize language model\n            self.prajna_model = PrajnaLanguageModel(model_type=\"demo\")\n            await self.prajna_model.load_model()\n            \n            logger.info(\"✅ Prajna components ready for PDF ingestion\")\n            \n        except Exception as e:\n            logger.error(f\"❌ Failed to initialize Prajna: {e}\")\n    \n    async def discover_pdfs(self) -> List[PDFDocument]:\n        \"\"\"Discover all PDF files in the data directory\"\"\"\n        logger.info(f\"🔍 Discovering PDF files in {self.data_dir}...\")\n        \n        discovered_pdfs = []\n        \n        # Recursively find all PDF files\n        for pdf_path in self.data_dir.rglob(\"*.pdf\"):\n            if not pdf_path.exists() or pdf_path.stat().st_size == 0:\n                continue\n            \n            # Calculate content hash\n            content_hash = self._calculate_file_hash(pdf_path)\n            \n            # Skip if already processed\n            if content_hash in self.processed_hashes:\n                continue\n            \n            # Extract basic PDF info\n            pdf_info = await self._extract_pdf_info(pdf_path)\n            \n            if pdf_info:\n                pdf_doc = PDFDocument(\n                    path=pdf_path,\n                    size=pdf_path.stat().st_size,\n                    pages=pdf_info.get('pages', 0),\n                    title=pdf_info.get('title', pdf_path.stem),\n                    author=pdf_info.get('author', 'Unknown'),\n                    subject=pdf_info.get('subject', ''),\n                    content_hash=content_hash\n                )\n                \n                discovered_pdfs.append(pdf_doc)\n        \n        # Sort by size (smaller PDFs first for faster initial processing)\n        discovered_pdfs.sort(key=lambda p: p.size)\n        \n        self.stats.total_pdfs = len(discovered_pdfs)\n        self.stats.total_pages = sum(p.pages for p in discovered_pdfs)\n        self.stats.total_size = sum(p.size for p in discovered_pdfs)\n        \n        logger.info(f\"📊 Discovered {len(discovered_pdfs)} PDF files\")\n        logger.info(f\"📊 Total pages: {self.stats.total_pages:,}\")\n        logger.info(f\"📊 Total size: {self._format_size(self.stats.total_size)}\")\n        \n        return discovered_pdfs\n    \n    def _calculate_file_hash(self, file_path: Path) -> str:\n        \"\"\"Calculate SHA-256 hash of file content\"\"\"\n        try:\n            hasher = hashlib.sha256()\n            with open(file_path, 'rb') as f:\n                for chunk in iter(lambda: f.read(8192), b\"\"):\n                    hasher.update(chunk)\n            return hasher.hexdigest()[:16]\n        except Exception:\n            return str(file_path)\n    \n    async def _extract_pdf_info(self, pdf_path: Path) -> Optional[Dict[str, Any]]:\n        \"\"\"Extract basic PDF information\"\"\"\n        try:\n            if PYMUPDF_AVAILABLE:\n                return await self._extract_with_pymupdf(pdf_path)\n            elif PDF_EXTRACTION_AVAILABLE:\n                return await self._extract_with_pypdf2(pdf_path)\n            else:\n                # Fallback - just file info\n                return {\n                    'pages': 0,\n                    'title': pdf_path.stem,\n                    'author': 'Unknown',\n                    'subject': ''\n                }\n        except Exception as e:\n            logger.warning(f\"⚠️ Failed to extract PDF info from {pdf_path}: {e}\")\n            return None\n    \n    async def _extract_with_pymupdf(self, pdf_path: Path) -> Dict[str, Any]:\n        \"\"\"Extract PDF info using PyMuPDF (recommended)\"\"\"\n        doc = fitz.open(str(pdf_path))\n        metadata = doc.metadata\n        \n        return {\n            'pages': doc.page_count,\n            'title': metadata.get('title', pdf_path.stem) or pdf_path.stem,\n            'author': metadata.get('author', 'Unknown') or 'Unknown',\n            'subject': metadata.get('subject', '') or '',\n            'creator': metadata.get('creator', ''),\n            'producer': metadata.get('producer', '')\n        }\n    \n    async def _extract_with_pypdf2(self, pdf_path: Path) -> Dict[str, Any]:\n        \"\"\"Extract PDF info using PyPDF2 (fallback)\"\"\"\n        with open(pdf_path, 'rb') as f:\n            pdf_reader = PyPDF2.PdfReader(f)\n            \n            info = pdf_reader.metadata if pdf_reader.metadata else {}\n            \n            return {\n                'pages': len(pdf_reader.pages),\n                'title': info.get('/Title', pdf_path.stem) or pdf_path.stem,\n                'author': info.get('/Author', 'Unknown') or 'Unknown',\n                'subject': info.get('/Subject', '') or ''\n            }\n    \n    async def ingest_pdfs(self, max_pdfs: Optional[int] = None) -> PDFIngestionStats:\n        \"\"\"Main PDF ingestion process\"\"\"\n        logger.info(\"📚 Starting PDF-only ingestion for Prajna's consciousness...\")\n        \n        self.stats.start_time = time.time()\n        \n        # Initialize Prajna\n        await self.initialize_prajna()\n        \n        # Discover PDFs\n        pdfs_to_process = await self.discover_pdfs()\n        \n        if max_pdfs:\n            pdfs_to_process = pdfs_to_process[:max_pdfs]\n            logger.info(f\"🔢 Limited to {max_pdfs} PDFs for this run\")\n        \n        # Process PDFs in batches\n        logger.info(f\"⚡ Processing {len(pdfs_to_process)} PDFs in batches of {self.batch_size}...\")\n        \n        for i in range(0, len(pdfs_to_process), self.batch_size):\n            batch = pdfs_to_process[i:i + self.batch_size]\n            \n            logger.info(f\"📦 Processing PDF batch {i//self.batch_size + 1}/{(len(pdfs_to_process)-1)//self.batch_size + 1}\")\n            \n            # Process batch with controlled concurrency\n            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n                tasks = []\n                for pdf_doc in batch:\n                    task = executor.submit(self._process_pdf_sync, pdf_doc)\n                    tasks.append(task)\n                \n                # Wait for batch completion\n                for task in concurrent.futures.as_completed(tasks):\n                    try:\n                        result = task.result()\n                        if result:\n                            self.stats.processed_pdfs += 1\n                            self.stats.processed_size += result.get('size', 0)\n                        else:\n                            self.stats.failed_pdfs += 1\n                    except Exception as e:\n                        self.stats.failed_pdfs += 1\n                        logger.error(f\"❌ PDF processing failed: {e}\")\n            \n            # Progress update\n            self._print_progress()\n            \n            # Longer delay between PDF batches (more intensive processing)\n            await asyncio.sleep(0.5)\n        \n        # Final statistics\n        elapsed_time = time.time() - self.stats.start_time\n        logger.info(f\"\\n🎉 PDF INGESTION COMPLETE!\")\n        logger.info(f\"📊 Processed: {self.stats.processed_pdfs}/{self.stats.total_pdfs} PDFs\")\n        logger.info(f\"📊 Total pages: {self.stats.total_pages:,}\")\n        logger.info(f\"📊 Data size: {self._format_size(self.stats.processed_size)}\")\n        logger.info(f\"📊 Time taken: {elapsed_time:.1f} seconds\")\n        logger.info(f\"📊 Rate: {self.stats.processing_rate:.1f} PDFs/second\")\n        logger.info(f\"📊 Failed: {self.stats.failed_pdfs} PDFs\")\n        \n        return self.stats\n    \n    def _process_pdf_sync(self, pdf_doc: PDFDocument) -> Optional[Dict[str, Any]]:\n        \"\"\"Synchronous PDF processing (for thread pool)\"\"\"\n        try:\n            return asyncio.run(self._process_pdf(pdf_doc))\n        except Exception as e:\n            logger.error(f\"❌ Failed to process PDF {pdf_doc.path}: {e}\")\n            return None\n    \n    async def _process_pdf(self, pdf_doc: PDFDocument) -> Optional[Dict[str, Any]]:\n        \"\"\"Process a single PDF document\"\"\"\n        try:\n            # Extract full text content\n            text_content = await self._extract_pdf_text(pdf_doc.path)\n            \n            if text_content:\n                pdf_doc.text_content = text_content[:20000]  # Limit to 20KB of text\n                \n                # Extract concepts from content\n                pdf_doc.concepts = await self._extract_concepts(text_content)\n                \n                # Determine document type\n                doc_type = self._classify_pdf_type(text_content, pdf_doc)\n            \n            # Create comprehensive PDF entry\n            pdf_entry = {\n                'file_path': str(pdf_doc.path),\n                'file_type': 'pdf_document',\n                'title': pdf_doc.title,\n                'author': pdf_doc.author,\n                'subject': pdf_doc.subject,\n                'pages': pdf_doc.pages,\n                'text_content': pdf_doc.text_content,\n                'concepts': pdf_doc.concepts,\n                'document_type': doc_type,\n                'content_hash': pdf_doc.content_hash,\n                'size': pdf_doc.size,\n                'ingestion_time': datetime.now().isoformat()\n            }\n            \n            # Store in Prajna's memory systems\n            if PRAJNA_AVAILABLE and self.soliton_memory:\n                await self._store_pdf_in_memory(pdf_entry)\n            \n            # Mark as processed\n            self.processed_hashes.add(pdf_doc.content_hash)\n            \n            return {'size': pdf_doc.size, 'pages': pdf_doc.pages}\n            \n        except Exception as e:\n            logger.warning(f\"⚠️ Failed to process PDF {pdf_doc.path}: {e}\")\n            return None\n    \n    async def _extract_pdf_text(self, pdf_path: Path) -> str:\n        \"\"\"Extract text content from PDF\"\"\"\n        try:\n            if PYMUPDF_AVAILABLE:\n                return await self._extract_text_pymupdf(pdf_path)\n            elif PDF_EXTRACTION_AVAILABLE:\n                return await self._extract_text_pypdf2(pdf_path)\n            else:\n                return f\"PDF document: {pdf_path.name} (text extraction not available)\"\n        except Exception as e:\n            logger.warning(f\"⚠️ Text extraction failed for {pdf_path}: {e}\")\n            return f\"PDF document: {pdf_path.name} (extraction failed)\"\n    \n    async def _extract_text_pymupdf(self, pdf_path: Path) -> str:\n        \"\"\"Extract text using PyMuPDF\"\"\"\n        doc = fitz.open(str(pdf_path))\n        text_content = \"\"\n        \n        # Extract text from each page (limit to first 50 pages for performance)\n        for page_num in range(min(50, doc.page_count)):\n            page = doc[page_num]\n            text_content += page.get_text() + \"\\n\\n\"\n        \n        doc.close()\n        return text_content\n    \n    async def _extract_text_pypdf2(self, pdf_path: Path) -> str:\n        \"\"\"Extract text using PyPDF2\"\"\"\n        with open(pdf_path, 'rb') as f:\n            pdf_reader = PyPDF2.PdfReader(f)\n            text_content = \"\"\n            \n            # Extract text from each page (limit to first 50 pages)\n            for page_num in range(min(50, len(pdf_reader.pages))):\n                page = pdf_reader.pages[page_num]\n                text_content += page.extract_text() + \"\\n\\n\"\n        \n        return text_content\n    \n    async def _extract_concepts(self, text_content: str) -> List[str]:\n        \"\"\"Extract key concepts from PDF text content\"\"\"\n        concepts = set()\n        \n        # Extract using concept patterns\n        for pattern in self.concept_patterns:\n            matches = re.findall(pattern, text_content, re.IGNORECASE)\n            concepts.update([match.lower().strip() for match in matches if len(match) > 3])\n        \n        # Add document-specific concepts\n        text_lower = text_content.lower()\n        \n        # Academic/research indicators\n        if any(re.search(pattern, text_lower) for pattern in self.academic_patterns):\n            concepts.add('academic_paper')\n            concepts.add('research')\n        \n        # Technology indicators\n        tech_terms = ['ai', 'machine learning', 'neural network', 'algorithm', 'data', 'model']\n        for term in tech_terms:\n            if term in text_lower:\n                concepts.add(term.replace(' ', '_'))\n        \n        return list(concepts)[:50]  # Limit to top 50 concepts\n    \n    def _classify_pdf_type(self, text_content: str, pdf_doc: PDFDocument) -> str:\n        \"\"\"Classify the type of PDF document\"\"\"\n        text_lower = text_content.lower()\n        \n        # Academic paper\n        if any(re.search(pattern, text_lower) for pattern in self.academic_patterns):\n            return 'academic_paper'\n        \n        # Technical documentation\n        if any(word in text_lower for word in ['api', 'documentation', 'manual', 'guide']):\n            return 'technical_documentation'\n        \n        # Book/ebook\n        if pdf_doc.pages > 100 and any(word in text_lower for word in ['chapter', 'table of contents']):\n            return 'book'\n        \n        # Report\n        if any(word in text_lower for word in ['report', 'analysis', 'findings', 'executive summary']):\n            return 'report'\n        \n        # Presentation\n        if pdf_doc.pages < 50 and any(word in text_lower for word in ['slide', 'presentation']):\n            return 'presentation'\n        \n        return 'general_document'\n    \n    async def _store_pdf_in_memory(self, pdf_entry: Dict[str, Any]):\n        \"\"\"Store PDF entry in Prajna's memory systems\"\"\"\n        try:\n            # Store in Soliton Memory\n            # In production, this would call actual memory APIs\n            pass\n        except Exception as e:\n            logger.warning(f\"⚠️ Memory storage failed: {e}\")\n    \n    def _print_progress(self):\n        \"\"\"Print PDF ingestion progress\"\"\"\n        elapsed = time.time() - self.stats.start_time\n        rate = self.stats.processing_rate\n        \n        print(f\"\\r📚 PDF Progress: {self.stats.progress_percent:.1f}% | \"\n              f\"PDFs: {self.stats.processed_pdfs}/{self.stats.total_pdfs} | \"\n              f\"Rate: {rate:.1f}/sec | \"\n              f\"Failed: {self.stats.failed_pdfs} | \"\n              f\"Time: {elapsed:.0f}s\", end=\"\", flush=True)\n    \n    def _format_size(self, size_bytes: int) -> str:\n        \"\"\"Format file size in human readable form\"\"\"\n        for unit in ['B', 'KB', 'MB', 'GB']:\n            if size_bytes < 1024:\n                return f\"{size_bytes:.1f} {unit}\"\n            size_bytes /= 1024\n        return f\"{size_bytes:.1f} TB\"\n\nasync def main():\n    \"\"\"Main PDF ingestion function\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Ingest PDF documents into Prajna\")\n    parser.add_argument(\"--data-dir\", default=\"C:\\\\Users\\\\jason\\\\Desktop\\\\tori\\\\kha\\\\data\",\n                       help=\"Data directory to scan for PDFs\")\n    parser.add_argument(\"--max-pdfs\", type=int, help=\"Maximum PDFs to process\")\n    parser.add_argument(\"--batch-size\", type=int, default=25, help=\"PDF batch size\")\n    parser.add_argument(\"--workers\", type=int, default=3, help=\"Number of workers\")\n    \n    args = parser.parse_args()\n    \n    # Create PDF ingestor\n    ingestor = PrajnaPDFIngestor(args.data_dir)\n    ingestor.batch_size = args.batch_size\n    ingestor.max_workers = args.workers\n    \n    # Run PDF ingestion\n    stats = await ingestor.ingest_pdfs(max_pdfs=args.max_pdfs)\n    \n    print(f\"\\n\\n🎉 PRAJNA PDF INGESTION COMPLETE!\")\n    print(f\"📚 Prajna's consciousness has absorbed:\")\n    print(f\"   📄 {stats.processed_pdfs} PDF documents\")\n    print(f\"   📃 {stats.total_pages:,} total pages\")\n    print(f\"   💾 {ingestor._format_size(stats.processed_size)} of PDF data\")\n    print(f\"   ⚡ {stats.processing_rate:.1f} PDFs per second\")\n    print(f\"   ❌ {stats.failed_pdfs} failed PDFs\")\n    print(f\"\\n🧠 Prajna is now ready to reason with PDF knowledge!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n