#!/usr/bin/env python3\n\"\"\"\nAdvanced PDF Processor for Prajna\n=================================\n\nSpecialized advanced processing of PDF documents with:\n- Academic paper recognition and analysis\n- Technical document classification\n- Research metadata extraction\n- Citation analysis\n- Abstract and conclusion extraction\n\"\"\"\n\nimport asyncio\nimport logging\nimport json\nimport re\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\ntry:\n    import fitz  # PyMuPDF\n    PYMUPDF_AVAILABLE = True\nexcept ImportError:\n    PYMUPDF_AVAILABLE = False\n\ntry:\n    import PyPDF2\n    PYPDF2_AVAILABLE = True\nexcept ImportError:\n    PYPDF2_AVAILABLE = False\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"prajna.advanced_pdf\")\n\nclass AdvancedPDFProcessor:\n    \"\"\"Advanced PDF processing with academic and technical document analysis\"\"\"\n    \n    def __init__(self, data_directory: str):\n        self.data_dir = Path(data_directory)\n        self.processed_count = 0\n        self.pdf_knowledge = []\n        \n        # Academic paper patterns\n        self.academic_sections = {\n            'abstract': r'\\b(?:abstract|summary)\\b[:\\s]*([^\\n]{100,1000})',\n            'introduction': r'\\b(?:introduction|background)\\b[:\\s]*([^\\n]{100,1000})',\n            'methodology': r'\\b(?:method(?:ology)?|approach|technique)\\b[:\\s]*([^\\n]{100,1000})',\n            'results': r'\\b(?:results?|findings?)\\b[:\\s]*([^\\n]{100,1000})',\n            'conclusion': r'\\b(?:conclusion|summary)\\b[:\\s]*([^\\n]{100,1000})'\n        }\n        \n        # Research field indicators\n        self.research_fields = {\n            'artificial_intelligence': ['ai', 'artificial intelligence', 'machine learning', 'neural network', 'deep learning'],\n            'computer_science': ['algorithm', 'programming', 'software', 'computing', 'file_storage'],\n            'data_science': ['data mining', 'statistics', 'analytics', 'big data', 'visualization'],\n            'physics': ['quantum', 'particle', 'relativity', 'thermodynamics', 'mechanics'],\n            'biology': ['gene', 'protein', 'cell', 'molecular', 'evolution'],\n            'chemistry': ['molecule', 'chemical', 'reaction', 'synthesis', 'compound'],\n            'mathematics': ['theorem', 'proof', 'equation', 'mathematical', 'formula'],\n            'engineering': ['design', 'system', 'optimization', 'control', 'automation']\n        }\n        \n        # Citation patterns\n        self.citation_patterns = [\n            r'\\[\\d+\\]',  # [1], [2], etc.\n            r'\\([^)]*\\d{4}[^)]*\\)',  # (Author, 2023)\n            r'(?:et al\\.?,?\\s*\\d{4})',  # et al. 2023\n        ]\n    \n    async def process_all_pdfs(self):\n        \"\"\"Process all PDFs with advanced analysis\"\"\"\n        logger.info(f\"üìö Advanced PDF processing of {self.data_dir}...\")\n        \n        for pdf_path in self.data_dir.rglob(\"*.pdf\"):\n            if pdf_path.exists() and pdf_path.stat().st_size > 0:\n                await self._process_pdf_advanced(pdf_path)\n        \n        await self._save_pdf_knowledge_base()\n        \n        logger.info(f\"‚úÖ Advanced processing complete: {self.processed_count} PDFs\")\n        logger.info(f\"üìö PDF knowledge base contains {len(self.pdf_knowledge)} entries\")\n    \n    async def _process_pdf_advanced(self, pdf_path: Path):\n        \"\"\"Advanced processing of individual PDF\"\"\"\n        try:\n            logger.debug(f\"üî¨ Advanced analysis: {pdf_path.name}\")\n            \n            # Extract comprehensive PDF data\n            pdf_data = await self._extract_comprehensive_pdf_data(pdf_path)\n            \n            if pdf_data:\n                # Perform advanced analysis\n                analysis = await self._perform_advanced_analysis(pdf_data)\n                \n                # Create comprehensive knowledge entry\n                knowledge_entry = {\n                    **pdf_data,\n                    **analysis,\n                    'processing_type': 'advanced_analysis',\n                    'processed_at': datetime.now().isoformat()\n                }\n                \n                self.pdf_knowledge.append(knowledge_entry)\n                self.processed_count += 1\n                \n                if self.processed_count % 10 == 0:\n                    logger.info(f\"üìä Advanced processed {self.processed_count} PDFs...\")\n                    \n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è Advanced processing failed for {pdf_path}: {e}\")\n    \n    async def _extract_comprehensive_pdf_data(self, pdf_path: Path) -> Optional[Dict[str, Any]]:\n        \"\"\"Extract comprehensive data from PDF\"\"\"\n        try:\n            if PYMUPDF_AVAILABLE:\n                return await self._extract_with_pymupdf_advanced(pdf_path)\n            elif PYPDF2_AVAILABLE:\n                return await self._extract_with_pypdf2_advanced(pdf_path)\n            else:\n                return None\n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è PDF extraction failed for {pdf_path}: {e}\")\n            return None\n    \n    async def _extract_with_pymupdf_advanced(self, pdf_path: Path) -> Dict[str, Any]:\n        \"\"\"Advanced extraction using PyMuPDF\"\"\"\n        doc = fitz.open(str(pdf_path))\n        \n        # Extract metadata\n        metadata = doc.metadata\n        \n        # Extract full text content\n        full_text = \"\"\n        for page_num in range(min(100, doc.page_count)):  # Limit to 100 pages\n            page = doc[page_num]\n            full_text += page.get_text() + \"\\n\\n\"\n        \n        doc.close()\n        \n        return {\n            'path': str(pdf_path),\n            'filename': pdf_path.name,\n            'size': pdf_path.stat().st_size,\n            'pages': doc.page_count,\n            'title': metadata.get('title', pdf_path.stem) or pdf_path.stem,\n            'author': metadata.get('author', '') or '',\n            'subject': metadata.get('subject', '') or '',\n            'creator': metadata.get('creator', '') or '',\n            'producer': metadata.get('producer', '') or '',\n            'creation_date': metadata.get('creationDate', '') or '',\n            'modification_date': metadata.get('modDate', '') or '',\n            'full_text': full_text[:50000],  # Limit to 50KB\n            'text_length': len(full_text)\n        }\n    \n    async def _extract_with_pypdf2_advanced(self, pdf_path: Path) -> Dict[str, Any]:\n        \"\"\"Advanced extraction using PyPDF2\"\"\"\n        with open(pdf_path, 'rb') as f:\n            pdf_reader = PyPDF2.PdfReader(f)\n            \n            # Extract metadata\n            info = pdf_reader.metadata if pdf_reader.metadata else {}\n            \n            # Extract text content\n            full_text = \"\"\n            for page_num in range(min(100, len(pdf_reader.pages))):\n                page = pdf_reader.pages[page_num]\n                full_text += page.extract_text() + \"\\n\\n\"\n        \n        return {\n            'path': str(pdf_path),\n            'filename': pdf_path.name,\n            'size': pdf_path.stat().st_size,\n            'pages': len(pdf_reader.pages),\n            'title': info.get('/Title', pdf_path.stem) or pdf_path.stem,\n            'author': info.get('/Author', '') or '',\n            'subject': info.get('/Subject', '') or '',\n            'creator': info.get('/Creator', '') or '',\n            'producer': info.get('/Producer', '') or '',\n            'creation_date': str(info.get('/CreationDate', '')) or '',\n            'modification_date': str(info.get('/ModDate', '')) or '',\n            'full_text': full_text[:50000],\n            'text_length': len(full_text)\n        }\n    \n    async def _perform_advanced_analysis(self, pdf_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Perform advanced analysis on PDF data\"\"\"\n        text = pdf_data.get('full_text', '').lower()\n        \n        analysis = {\n            'document_type': await self._classify_document_type(pdf_data),\n            'research_field': await self._identify_research_field(text),\n            'academic_sections': await self._extract_academic_sections(text),\n            'citations_count': await self._count_citations(text),\n            'key_concepts': await self._extract_key_concepts(text),\n            'reading_level': await self._estimate_reading_level(text),\n            'language_analysis': await self._analyze_language(text)\n        }\n        \n        return analysis\n    \n    async def _classify_document_type(self, pdf_data: Dict[str, Any]) -> str:\n        \"\"\"Classify the type of PDF document\"\"\"\n        text = pdf_data.get('full_text', '').lower()\n        title = pdf_data.get('title', '').lower()\n        pages = pdf_data.get('pages', 0)\n        \n        # Academic paper indicators\n        academic_keywords = ['abstract', 'introduction', 'methodology', 'results', 'conclusion', 'references']\n        academic_score = sum(1 for keyword in academic_keywords if keyword in text)\n        \n        if academic_score >= 4:\n            return 'academic_paper'\n        \n        # Technical documentation\n        if any(word in text for word in ['api', 'documentation', 'manual', 'specification']):\n            return 'technical_documentation'\n        \n        # Book/ebook\n        if pages > 100 and any(word in text for word in ['chapter', 'table of contents', 'index']):\n            return 'book'\n        \n        # Conference paper\n        if any(word in title for word in ['proceedings', 'conference', 'workshop']):\n            return 'conference_paper'\n        \n        # Thesis/dissertation\n        if any(word in title for word in ['thesis', 'dissertation', 'phd']):\n            return 'thesis'\n        \n        # Report\n        if any(word in text for word in ['report', 'findings', 'executive summary']):\n            return 'report'\n        \n        # Patent\n        if any(word in text for word in ['patent', 'invention', 'claim']):\n            return 'patent'\n        \n        return 'general_document'\n    \n    async def _identify_research_field(self, text: str) -> List[str]:\n        \"\"\"Identify research fields based on content\"\"\"\n        identified_fields = []\n        \n        for field, keywords in self.research_fields.items():\n            score = sum(1 for keyword in keywords if keyword in text)\n            if score >= 2:  # At least 2 keywords from the field\n                identified_fields.append(field)\n        \n        return identified_fields\n    \n    async def _extract_academic_sections(self, text: str) -> Dict[str, str]:\n        \"\"\"Extract academic paper sections\"\"\"\n        sections = {}\n        \n        for section_name, pattern in self.academic_sections.items():\n            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n            if matches:\n                # Take the first match and clean it up\n                content = matches[0].strip()[:500]  # Limit to 500 chars\n                sections[section_name] = content\n        \n        return sections\n    \n    async def _count_citations(self, text: str) -> int:\n        \"\"\"Count citations in the document\"\"\"\n        citation_count = 0\n        \n        for pattern in self.citation_patterns:\n            matches = re.findall(pattern, text)\n            citation_count += len(matches)\n        \n        return citation_count\n    \n    async def _extract_key_concepts(self, text: str) -> List[str]:\n        \"\"\"Extract key concepts from text\"\"\"\n        # Simple concept extraction\n        words = text.split()\n        \n        # Filter for important words (length > 5, not common words)\n        important_words = []\n        common_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'way', 'use', 'man', 'say', 'she', 'too', 'any', 'here', 'much', 'well', 'back', 'been', 'call', 'came', 'each', 'find', 'good', 'hand', 'have', 'just', 'know', 'last', 'left', 'life', 'live', 'look', 'made', 'make', 'most', 'move', 'must', 'name', 'need', 'only', 'over', 'part', 'place', 'right', 'said', 'same', 'seem', 'show', 'side', 'take', 'tell', 'than', 'that', 'them', 'they', 'this', 'time', 'very', 'want', 'water', 'will', 'with', 'word', 'work', 'year', 'where', 'would', 'write'}\n        \n        for word in words:\n            clean_word = re.sub(r'[^a-zA-Z]', '', word.lower())\n            if len(clean_word) > 5 and clean_word not in common_words:\n                important_words.append(clean_word)\n        \n        # Count frequency and return top concepts\n        word_freq = {}\n        for word in important_words:\n            word_freq[word] = word_freq.get(word, 0) + 1\n        \n        # Return top 20 most frequent concepts\n        sorted_concepts = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n        return [concept[0] for concept in sorted_concepts[:20]]\n    \n    async def _estimate_reading_level(self, text: str) -> str:\n        \"\"\"Estimate reading level of the document\"\"\"\n        # Simple heuristic based on average sentence length and word length\n        sentences = text.split('.')\n        words = text.split()\n        \n        if len(sentences) == 0 or len(words) == 0:\n            return 'unknown'\n        \n        avg_sentence_length = len(words) / len(sentences)\n        avg_word_length = sum(len(word) for word in words) / len(words)\n        \n        # Simple classification\n        if avg_sentence_length > 20 and avg_word_length > 6:\n            return 'graduate_level'\n        elif avg_sentence_length > 15 and avg_word_length > 5:\n            return 'undergraduate_level'\n        elif avg_sentence_length > 10:\n            return 'high_school_level'\n        else:\n            return 'general_public'\n    \n    async def _analyze_language(self, text: str) -> Dict[str, Any]:\n        \"\"\"Analyze language characteristics\"\"\"\n        # Basic language analysis\n        words = text.split()\n        \n        return {\n            'word_count': len(words),\n            'unique_words': len(set(words)),\n            'avg_word_length': sum(len(word) for word in words) / len(words) if words else 0,\n            'technical_density': self._calculate_technical_density(text),\n            'formality_score': self._calculate_formality_score(text)\n        }\n    \n    def _calculate_technical_density(self, text: str) -> float:\n        \"\"\"Calculate density of technical terms\"\"\"\n        technical_indicators = ['algorithm', 'methodology', 'framework', 'implementation', 'optimization', 'analysis']\n        words = text.lower().split()\n        technical_count = sum(1 for word in words if any(indicator in word for indicator in technical_indicators))\n        \n        return technical_count / len(words) if words else 0\n    \n    def _calculate_formality_score(self, text: str) -> float:\n        \"\"\"Calculate formality score\"\"\"\n        formal_indicators = ['furthermore', 'however', 'therefore', 'consequently', 'moreover', 'nevertheless']\n        informal_indicators = ['really', 'pretty', 'quite', 'very', 'pretty much', 'kind of']\n        \n        text_lower = text.lower()\n        formal_count = sum(1 for indicator in formal_indicators if indicator in text_lower)\n        informal_count = sum(1 for indicator in informal_indicators if indicator in text_lower)\n        \n        total_indicators = formal_count + informal_count\n        if total_indicators == 0:\n            return 0.5  # Neutral\n        \n        return formal_count / total_indicators\n    \n    async def _save_pdf_knowledge_base(self):\n        \"\"\"Save advanced PDF knowledge base\"\"\"\n        output_file = Path(\"prajna_pdf_knowledge.json\")\n        \n        # Organize data by document types\n        document_types = {}\n        for pdf in self.pdf_knowledge:\n            doc_type = pdf.get('document_type', 'unknown')\n            if doc_type not in document_types:\n                document_types[doc_type] = []\n            document_types[doc_type].append(pdf)\n        \n        # Create comprehensive knowledge structure\n        knowledge_summary = {\n            'metadata': {\n                'total_pdfs': len(self.pdf_knowledge),\n                'processing_date': datetime.now().isoformat(),\n                'source_directory': str(self.data_dir),\n                'processing_type': 'advanced_analysis'\n            },\n            'document_types': {doc_type: len(docs) for doc_type, docs in document_types.items()},\n            'pdf_documents': self.pdf_knowledge\n        }\n        \n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(knowledge_summary, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"üíæ Advanced PDF knowledge base saved: {output_file}\")\n        \n        # Create concepts and fields index\n        await self._create_pdf_concepts_index()\n    \n    async def _create_pdf_concepts_index(self):\n        \"\"\"Create index of concepts and research fields\"\"\"\n        all_concepts = {}\n        research_fields = {}\n        document_types = {}\n        \n        for pdf in self.pdf_knowledge:\n            # Collect concepts\n            for concept in pdf.get('key_concepts', []):\n                all_concepts[concept] = all_concepts.get(concept, 0) + 1\n            \n            # Collect research fields\n            for field in pdf.get('research_field', []):\n                research_fields[field] = research_fields.get(field, 0) + 1\n            \n            # Collect document types\n            doc_type = pdf.get('document_type', 'unknown')\n            document_types[doc_type] = document_types.get(doc_type, 0) + 1\n        \n        concepts_index = {\n            'total_unique_concepts': len(all_concepts),\n            'total_research_fields': len(research_fields),\n            'concepts': dict(sorted(all_concepts.items(), key=lambda x: x[1], reverse=True)),\n            'research_fields': dict(sorted(research_fields.items(), key=lambda x: x[1], reverse=True)),\n            'document_types': dict(sorted(document_types.items(), key=lambda x: x[1], reverse=True))\n        }\n        \n        with open('prajna_pdf_concepts.json', 'w', encoding='utf-8') as f:\n            json.dump(concepts_index, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"üß† PDF concepts index created: {len(all_concepts)} concepts, {len(research_fields)} fields\")\n        logger.info(f\"üìö Document types: {list(document_types.keys())}\")\n        logger.info(f\"üî¨ Research fields: {list(research_fields.keys())}\")\n\nasync def main():\n    \"\"\"Main function for advanced PDF processing\"\"\"\n    data_directory = \"C:\\\\Users\\\\jason\\\\Desktop\\\\tori\\\\kha\\\\data\"\n    \n    if not PYMUPDF_AVAILABLE and not PYPDF2_AVAILABLE:\n        print(\"‚ùå No PDF processing libraries available!\")\n        print(\"Install with: pip install PyMuPDF PyPDF2\")\n        return\n    \n    processor = AdvancedPDFProcessor(data_directory)\n    await processor.process_all_pdfs()\n    \n    print(\"\\nüéâ ADVANCED PDF PROCESSING COMPLETE!\")\n    print(\"üìÅ Files created:\")\n    print(\"   üìö prajna_pdf_knowledge.json - Complete PDF knowledge base\")\n    print(\"   üß† prajna_pdf_concepts.json - PDF concepts and research fields index\")\n    print(\"\\nüìä PDF Analysis Features:\")\n    print(\"   ‚úÖ Document type classification\")\n    print(\"   ‚úÖ Research field identification\")\n    print(\"   ‚úÖ Academic section extraction\")\n    print(\"   ‚úÖ Citation counting\")\n    print(\"   ‚úÖ Key concept extraction\")\n    print(\"   ‚úÖ Reading level estimation\")\n    print(\"   ‚úÖ Language analysis\")\n    print(\"\\nüß† Prajna's PDF consciousness has been dramatically enhanced!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n