#!/usr/bin/env python3\n\"\"\"\nAdvanced PDF Processor for Prajna\n=================================\n\nSpecialized advanced processing of PDF documents with:\n- Academic paper recognition and analysis\n- Technical document classification\n- Research metadata extraction\n- Citation analysis\n- Abstract and conclusion extraction\n\"\"\"\n\nimport asyncio\nimport logging\nimport json\nimport re\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\ntry:\n    import fitz  # PyMuPDF\n    PYMUPDF_AVAILABLE = True\nexcept ImportError:\n    PYMUPDF_AVAILABLE = False\n\ntry:\n    import PyPDF2\n    PYPDF2_AVAILABLE = True\nexcept ImportError:\n    PYPDF2_AVAILABLE = False\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"prajna.advanced_pdf\")\n\nclass AdvancedPDFProcessor:\n    \"\"\"Advanced PDF processing with academic and technical document analysis\"\"\"\n    \n    def __init__(self, data_directory: str):\n        self.data_dir = Path(data_directory)\n        self.processed_count = 0\n        self.pdf_knowledge = []\n        \n        # Academic paper patterns\n        self.academic_sections = {\n            'abstract': r'\\b(?:abstract|summary)\\b[:\\s]*([^\\n]{100,1000})',\n            'introduction': r'\\b(?:introduction|background)\\b[:\\s]*([^\\n]{100,1000})',\n            'methodology': r'\\b(?:method(?:ology)?|approach|technique)\\b[:\\s]*([^\\n]{100,1000})',\n            'results': r'\\b(?:results?|findings?)\\b[:\\s]*([^\\n]{100,1000})',\n            'conclusion': r'\\b(?:conclusion|summary)\\b[:\\s]*([^\\n]{100,1000})'\n        }\n        \n        # Research field indicators\n        self.research_fields = {\n            'artificial_intelligence': ['ai', 'artificial intelligence', 'machine learning', 'neural network', 'deep learning'],\n            'computer_science': ['algorithm', 'programming', 'software', 'computing', 'file_storage'],\n            'data_science': ['data mining', 'statistics', 'analytics', 'big data', 'visualization'],\n            'physics': ['quantum', 'particle', 'relativity', 'thermodynamics', 'mechanics'],\n            'biology': ['gene', 'protein', 'cell', 'molecular', 'evolution'],\n            'chemistry': ['molecule', 'chemical', 'reaction', 'synthesis', 'compound'],\n            'mathematics': ['theorem', 'proof', 'equation', 'mathematical', 'formula'],\n            'engineering': ['design', 'system', 'optimization', 'control', 'automation']\n        }\n        \n        # Citation patterns\n        self.citation_patterns = [\n            r'\\[\\d+\\]',  # [1], [2], etc.\n            r'\\([^)]*\\d{4}[^)]*\\)',  # (Author, 2023)\n            r'(?:et al\\.?,?\\s*\\d{4})',  # et al. 2023\n        ]\n    \n    async def process_all_pdfs(self):\n        \"\"\"Process all PDFs with advanced analysis\"\"\"\n        logger.info(f\"📚 Advanced PDF processing of {self.data_dir}...\")\n        \n        for pdf_path in self.data_dir.rglob(\"*.pdf\"):\n            if pdf_path.exists() and pdf_path.stat().st_size > 0:\n                await self._process_pdf_advanced(pdf_path)\n        \n        await self._save_pdf_knowledge_base()\n        \n        logger.info(f\"✅ Advanced processing complete: {self.processed_count} PDFs\")\n        logger.info(f\"📚 PDF knowledge base contains {len(self.pdf_knowledge)} entries\")\n    \n    async def _process_pdf_advanced(self, pdf_path: Path):\n        \"\"\"Advanced processing of individual PDF\"\"\"\n        try:\n            logger.debug(f\"🔬 Advanced analysis: {pdf_path.name}\")\n            \n            # Extract comprehensive PDF data\n            pdf_data = await self._extract_comprehensive_pdf_data(pdf_path)\n            \n            if pdf_data:\n                # Perform advanced analysis\n                analysis = await self._perform_advanced_analysis(pdf_data)\n                \n                # Create comprehensive knowledge entry\n                knowledge_entry = {\n                    **pdf_data,\n                    **analysis,\n                    'processing_type': 'advanced_analysis',\n                    'processed_at': datetime.now().isoformat()\n                }\n                \n                self.pdf_knowledge.append(knowledge_entry)\n                self.processed_count += 1\n                \n                if self.processed_count % 10 == 0:\n                    logger.info(f\"📊 Advanced processed {self.processed_count} PDFs...\")\n                    \n        except Exception as e:\n            logger.warning(f\"⚠️ Advanced processing failed for {pdf_path}: {e}\")\n    \n    async def _extract_comprehensive_pdf_data(self, pdf_path: Path) -> Optional[Dict[str, Any]]:\n        \"\"\"Extract comprehensive data from PDF\"\"\"\n        try:\n            if PYMUPDF_AVAILABLE:\n                return await self._extract_with_pymupdf_advanced(pdf_path)\n            elif PYPDF2_AVAILABLE:\n                return await self._extract_with_pypdf2_advanced(pdf_path)\n            else:\n                return None\n        except Exception as e:\n            logger.warning(f\"⚠️ PDF extraction failed for {pdf_path}: {e}\")\n            return None\n    \n    async def _extract_with_pymupdf_advanced(self, pdf_path: Path) -> Dict[str, Any]:\n        \"\"\"Advanced extraction using PyMuPDF\"\"\"\n        doc = fitz.open(str(pdf_path))\n        \n        # Extract metadata\n        metadata = doc.metadata\n        \n        # Extract full text content\n        full_text = \"\"\n        for page_num in range(min(100, doc.page_count)):  # Limit to 100 pages\n            page = doc[page_num]\n            full_text += page.get_text() + \"\\n\\n\"\n        \n        doc.close()\n        \n        return {\n            'path': str(pdf_path),\n            'filename': pdf_path.name,\n            'size': pdf_path.stat().st_size,\n            'pages': doc.page_count,\n            'title': metadata.get('title', pdf_path.stem) or pdf_path.stem,\n            'author': metadata.get('author', '') or '',\n            'subject': metadata.get('subject', '') or '',\n            'creator': metadata.get('creator', '') or '',\n            'producer': metadata.get('producer', '') or '',\n            'creation_date': metadata.get('creationDate', '') or '',\n            'modification_date': metadata.get('modDate', '') or '',\n            'full_text': full_text[:50000],  # Limit to 50KB\n            'text_length': len(full_text)\n        }\n    \n    async def _extract_with_pypdf2_advanced(self, pdf_path: Path) -> Dict[str, Any]:\n        \"\"\"Advanced extraction using PyPDF2\"\"\"\n        with open(pdf_path, 'rb') as f:\n            pdf_reader = PyPDF2.PdfReader(f)\n            \n            # Extract metadata\n            info = pdf_reader.metadata if pdf_reader.metadata else {}\n            \n            # Extract text content\n            full_text = \"\"\n            for page_num in range(min(100, len(pdf_reader.pages))):\n                page = pdf_reader.pages[page_num]\n                full_text += page.extract_text() + \"\\n\\n\"\n        \n        return {\n            'path': str(pdf_path),\n            'filename': pdf_path.name,\n            'size': pdf_path.stat().st_size,\n            'pages': len(pdf_reader.pages),\n            'title': info.get('/Title', pdf_path.stem) or pdf_path.stem,\n            'author': info.get('/Author', '') or '',\n            'subject': info.get('/Subject', '') or '',\n            'creator': info.get('/Creator', '') or '',\n            'producer': info.get('/Producer', '') or '',\n            'creation_date': str(info.get('/CreationDate', '')) or '',\n            'modification_date': str(info.get('/ModDate', '')) or '',\n            'full_text': full_text[:50000],\n            'text_length': len(full_text)\n        }\n    \n    async def _perform_advanced_analysis(self, pdf_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Perform advanced analysis on PDF data\"\"\"\n        text = pdf_data.get('full_text', '').lower()\n        \n        analysis = {\n            'document_type': await self._classify_document_type(pdf_data),\n            'research_field': await self._identify_research_field(text),\n            'academic_sections': await self._extract_academic_sections(text),\n            'citations_count': await self._count_citations(text),\n            'key_concepts': await self._extract_key_concepts(text),\n            'reading_level': await self._estimate_reading_level(text),\n            'language_analysis': await self._analyze_language(text)\n        }\n        \n        return analysis\n    \n    async def _classify_document_type(self, pdf_data: Dict[str, Any]) -> str:\n        \"\"\"Classify the type of PDF document\"\"\"\n        text = pdf_data.get('full_text', '').lower()\n        title = pdf_data.get('title', '').lower()\n        pages = pdf_data.get('pages', 0)\n        \n        # Academic paper indicators\n        academic_keywords = ['abstract', 'introduction', 'methodology', 'results', 'conclusion', 'references']\n        academic_score = sum(1 for keyword in academic_keywords if keyword in text)\n        \n        if academic_score >= 4:\n            return 'academic_paper'\n        \n        # Technical documentation\n        if any(word in text for word in ['api', 'documentation', 'manual', 'specification']):\n            return 'technical_documentation'\n        \n        # Book/ebook\n        if pages > 100 and any(word in text for word in ['chapter', 'table of contents', 'index']):\n            return 'book'\n        \n        # Conference paper\n        if any(word in title for word in ['proceedings', 'conference', 'workshop']):\n            return 'conference_paper'\n        \n        # Thesis/dissertation\n        if any(word in title for word in ['thesis', 'dissertation', 'phd']):\n            return 'thesis'\n        \n        # Report\n        if any(word in text for word in ['report', 'findings', 'executive summary']):\n            return 'report'\n        \n        # Patent\n        if any(word in text for word in ['patent', 'invention', 'claim']):\n            return 'patent'\n        \n        return 'general_document'\n    \n    async def _identify_research_field(self, text: str) -> List[str]:\n        \"\"\"Identify research fields based on content\"\"\"\n        identified_fields = []\n        \n        for field, keywords in self.research_fields.items():\n            score = sum(1 for keyword in keywords if keyword in text)\n            if score >= 2:  # At least 2 keywords from the field\n                identified_fields.append(field)\n        \n        return identified_fields\n    \n    async def _extract_academic_sections(self, text: str) -> Dict[str, str]:\n        \"\"\"Extract academic paper sections\"\"\"\n        sections = {}\n        \n        for section_name, pattern in self.academic_sections.items():\n            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n            if matches:\n                # Take the first match and clean it up\n                content = matches[0].strip()[:500]  # Limit to 500 chars\n                sections[section_name] = content\n        \n        return sections\n    \n    async def _count_citations(self, text: str) -> int:\n        \"\"\"Count citations in the document\"\"\"\n        citation_count = 0\n        \n        for pattern in self.citation_patterns:\n            matches = re.findall(pattern, text)\n            citation_count += len(matches)\n        \n        return citation_count\n    \n    async def _extract_key_concepts(self, text: str) -> List[str]:\n        \"\"\"Extract key concepts from text\"\"\"\n        # Simple concept extraction\n        words = text.split()\n        \n        # Filter for important words (length > 5, not common words)\n        important_words = []\n        common_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'way', 'use', 'man', 'say', 'she', 'too', 'any', 'here', 'much', 'well', 'back', 'been', 'call', 'came', 'each', 'find', 'good', 'hand', 'have', 'just', 'know', 'last', 'left', 'life', 'live', 'look', 'made', 'make', 'most', 'move', 'must', 'name', 'need', 'only', 'over', 'part', 'place', 'right', 'said', 'same', 'seem', 'show', 'side', 'take', 'tell', 'than', 'that', 'them', 'they', 'this', 'time', 'very', 'want', 'water', 'will', 'with', 'word', 'work', 'year', 'where', 'would', 'write'}\n        \n        for word in words:\n            clean_word = re.sub(r'[^a-zA-Z]', '', word.lower())\n            if len(clean_word) > 5 and clean_word not in common_words:\n                important_words.append(clean_word)\n        \n        # Count frequency and return top concepts\n        word_freq = {}\n        for word in important_words:\n            word_freq[word] = word_freq.get(word, 0) + 1\n        \n        # Return top 20 most frequent concepts\n        sorted_concepts = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n        return [concept[0] for concept in sorted_concepts[:20]]\n    \n    async def _estimate_reading_level(self, text: str) -> str:\n        \"\"\"Estimate reading level of the document\"\"\"\n        # Simple heuristic based on average sentence length and word length\n        sentences = text.split('.')\n        words = text.split()\n        \n        if len(sentences) == 0 or len(words) == 0:\n            return 'unknown'\n        \n        avg_sentence_length = len(words) / len(sentences)\n        avg_word_length = sum(len(word) for word in words) / len(words)\n        \n        # Simple classification\n        if avg_sentence_length > 20 and avg_word_length > 6:\n            return 'graduate_level'\n        elif avg_sentence_length > 15 and avg_word_length > 5:\n            return 'undergraduate_level'\n        elif avg_sentence_length > 10:\n            return 'high_school_level'\n        else:\n            return 'general_public'\n    \n    async def _analyze_language(self, text: str) -> Dict[str, Any]:\n        \"\"\"Analyze language characteristics\"\"\"\n        # Basic language analysis\n        words = text.split()\n        \n        return {\n            'word_count': len(words),\n            'unique_words': len(set(words)),\n            'avg_word_length': sum(len(word) for word in words) / len(words) if words else 0,\n            'technical_density': self._calculate_technical_density(text),\n            'formality_score': self._calculate_formality_score(text)\n        }\n    \n    def _calculate_technical_density(self, text: str) -> float:\n        \"\"\"Calculate density of technical terms\"\"\"\n        technical_indicators = ['algorithm', 'methodology', 'framework', 'implementation', 'optimization', 'analysis']\n        words = text.lower().split()\n        technical_count = sum(1 for word in words if any(indicator in word for indicator in technical_indicators))\n        \n        return technical_count / len(words) if words else 0\n    \n    def _calculate_formality_score(self, text: str) -> float:\n        \"\"\"Calculate formality score\"\"\"\n        formal_indicators = ['furthermore', 'however', 'therefore', 'consequently', 'moreover', 'nevertheless']\n        informal_indicators = ['really', 'pretty', 'quite', 'very', 'pretty much', 'kind of']\n        \n        text_lower = text.lower()\n        formal_count = sum(1 for indicator in formal_indicators if indicator in text_lower)\n        informal_count = sum(1 for indicator in informal_indicators if indicator in text_lower)\n        \n        total_indicators = formal_count + informal_count\n        if total_indicators == 0:\n            return 0.5  # Neutral\n        \n        return formal_count / total_indicators\n    \n    async def _save_pdf_knowledge_base(self):\n        \"\"\"Save advanced PDF knowledge base\"\"\"\n        output_file = Path(\"prajna_pdf_knowledge.json\")\n        \n        # Organize data by document types\n        document_types = {}\n        for pdf in self.pdf_knowledge:\n            doc_type = pdf.get('document_type', 'unknown')\n            if doc_type not in document_types:\n                document_types[doc_type] = []\n            document_types[doc_type].append(pdf)\n        \n        # Create comprehensive knowledge structure\n        knowledge_summary = {\n            'metadata': {\n                'total_pdfs': len(self.pdf_knowledge),\n                'processing_date': datetime.now().isoformat(),\n                'source_directory': str(self.data_dir),\n                'processing_type': 'advanced_analysis'\n            },\n            'document_types': {doc_type: len(docs) for doc_type, docs in document_types.items()},\n            'pdf_documents': self.pdf_knowledge\n        }\n        \n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(knowledge_summary, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"💾 Advanced PDF knowledge base saved: {output_file}\")\n        \n        # Create concepts and fields index\n        await self._create_pdf_concepts_index()\n    \n    async def _create_pdf_concepts_index(self):\n        \"\"\"Create index of concepts and research fields\"\"\"\n        all_concepts = {}\n        research_fields = {}\n        document_types = {}\n        \n        for pdf in self.pdf_knowledge:\n            # Collect concepts\n            for concept in pdf.get('key_concepts', []):\n                all_concepts[concept] = all_concepts.get(concept, 0) + 1\n            \n            # Collect research fields\n            for field in pdf.get('research_field', []):\n                research_fields[field] = research_fields.get(field, 0) + 1\n            \n            # Collect document types\n            doc_type = pdf.get('document_type', 'unknown')\n            document_types[doc_type] = document_types.get(doc_type, 0) + 1\n        \n        concepts_index = {\n            'total_unique_concepts': len(all_concepts),\n            'total_research_fields': len(research_fields),\n            'concepts': dict(sorted(all_concepts.items(), key=lambda x: x[1], reverse=True)),\n            'research_fields': dict(sorted(research_fields.items(), key=lambda x: x[1], reverse=True)),\n            'document_types': dict(sorted(document_types.items(), key=lambda x: x[1], reverse=True))\n        }\n        \n        with open('prajna_pdf_concepts.json', 'w', encoding='utf-8') as f:\n            json.dump(concepts_index, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"🧠 PDF concepts index created: {len(all_concepts)} concepts, {len(research_fields)} fields\")\n        logger.info(f\"📚 Document types: {list(document_types.keys())}\")\n        logger.info(f\"🔬 Research fields: {list(research_fields.keys())}\")\n\nasync def main():\n    \"\"\"Main function for advanced PDF processing\"\"\"\n    data_directory = \"C:\\\\Users\\\\jason\\\\Desktop\\\\tori\\\\kha\\\\data\"\n    \n    if not PYMUPDF_AVAILABLE and not PYPDF2_AVAILABLE:\n        print(\"❌ No PDF processing libraries available!\")\n        print(\"Install with: pip install PyMuPDF PyPDF2\")\n        return\n    \n    processor = AdvancedPDFProcessor(data_directory)\n    await processor.process_all_pdfs()\n    \n    print(\"\\n🎉 ADVANCED PDF PROCESSING COMPLETE!\")\n    print(\"📁 Files created:\")\n    print(\"   📚 prajna_pdf_knowledge.json - Complete PDF knowledge base\")\n    print(\"   🧠 prajna_pdf_concepts.json - PDF concepts and research fields index\")\n    print(\"\\n📊 PDF Analysis Features:\")\n    print(\"   ✅ Document type classification\")\n    print(\"   ✅ Research field identification\")\n    print(\"   ✅ Academic section extraction\")\n    print(\"   ✅ Citation counting\")\n    print(\"   ✅ Key concept extraction\")\n    print(\"   ✅ Reading level estimation\")\n    print(\"   ✅ Language analysis\")\n    print(\"\\n🧠 Prajna's PDF consciousness has been dramatically enhanced!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n