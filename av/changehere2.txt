Unified Patched changehere.txt with Phase A–G Fixes

─────────────────────────────────────────────────────────────────────────────
File: ${IRIS_ROOT}\ingest_bus\audio\ingest_audio.py
References:
- whisper (speech‐to‐text model)
- numpy, wave (spectral centroid)
- .emotion.detect_emotion (emotion + RMS)
─────────────────────────────────────────────────────────────────────────────
"""
Transcribe an audio file and attach spectral & emotion features.
"""
import whisper
import numpy as np
import wave
from .emotion import detect_emotion def transcribe_audio(audio_path: str) -> dict:
"""
Returns:
- transcript: str
- spectral_centroid: float
- rms: float
- emotion: str
"""
# 1) Speech‐to‐text
model = whisper.load_model("base.en")
result = model.transcribe(audio_path)
transcript = result["text"]
makefile
Copy
# 2) Spectral centroid (as before)
wf = wave.open(audio_path, "rb")
n_frames = wf.getnframes()
raw = wf.readframes(n_frames)
rate = wf.getframerate()
wf.close()

data = np.frombuffer(raw, dtype=np.int16)
mag = np.abs(np.fft.rfft(data.astype(np.float64)))
freqs = np.fft.rfftfreq(data.size, d=1.0 / rate)
centroid = float(np.sum(freqs * mag) / np.sum(mag))

# 3) Emotion + RMS
emo = detect_emotion(audio_path)

return {
    "transcript": transcript,
    "spectral_centroid": centroid,
    "rms": emo["rms"],
    "emotion": emo["emotion"],
}
─────────────────────────────────────────────────────────────────────────────
File: ${IRIS_ROOT}\ingest_bus\audio_init_.py
References:
- ingest_audio.transcribe_audio
─────────────────────────────────────────────────────────────────────────────
"""
Audio ingestion package: expose the main transcription API.
"""
from .ingest_audio import transcribe_audio all = ["transcribe_audio"]
─────────────────────────────────────────────────────────────────────────────
File: tests/audio/test_transcribe.py
References:
- tori.kha.ingest_bus.audio.transcribe_audio
- whisper.load_model (monkeypatched stub)
- numpy, wave (for sample WAV generation)
─────────────────────────────────────────────────────────────────────────────
import wave
import numpy as np
import pytest from tori.kha.ingest_bus.audio.ingest_audio import transcribe_audio def create_sample_wav(path, freq=440, duration=1.0, rate=16000):
"""
Generate a simple WAV file of a sine wave at given freq.
"""
t = np.linspace(0, duration, int(rate * duration), endpoint=False)
# amplitude 32767 to utilize full int16 range
tone = (0.5 * np.sin(2 * np.pi * freq * t) * 32767).astype(np.int16)
with wave.open(path, "wb") as wf:
wf.setnchannels(1)
wf.setsampwidth(2)
wf.setframerate(rate)
wf.writeframes(tone.tobytes()) @pytest.fixture(scope="module")
def sample_wav(tmp_path_factory, monkeypatch):
# 1) Create a temporary WAV file
samp_dir = tmp_path_factory.mktemp("samples")
samp_file = samp_dir / "hello.wav"
create_sample_wav(str(samp_file))
python
Copy
# 2) Stub whisper.load_model to return a dummy model
class DummyModel:
    def transcribe(self, path):
        return {"text": "hello world"}

monkeypatch.setattr(whisper, "load_model", lambda name: DummyModel())
return str(samp_file)
def test_transcribe_audio_structure(sample_wav):
result = transcribe_audio(sample_wav)
# Must be a dict with our keys
assert isinstance(result, dict)
assert "transcript" in result and isinstance(result["transcript"], str)
assert "spectral_centroid" in result and isinstance(result["spectral_centroid"], float)
# Check stub transcript correctness
assert result["transcript"] == "hello world"
─────────────────────────────────────────────────────────────────────────────
File: ${IRIS_ROOT}\ingest_bus\audio\emotion.py
References:
- numpy (FFT, array math)
- wave (read PCM frames)
─────────────────────────────────────────────────────────────────────────────
"""
Compute low-level audio features and classify basic emotion from a WAV.
"""
import numpy as np
import wave def compute_spectral_centroid(data: np.ndarray, rate: int) -> float:
"""
Spectral centroid = Σ(f_i * M_i) / Σ(M_i),
where M_i is magnitude at bin i, f_i is its frequency.
"""
mag = np.abs(np.fft.rfft(data.astype(np.float64)))
freqs = np.fft.rfftfreq(data.size, d=1.0 / rate)
return float(np.sum(freqs * mag) / np.sum(mag)) def compute_rms(data: np.ndarray) -> float:
"""
Root-mean-square energy = sqrt(mean(data^2)).
"""
return float(np.sqrt(np.mean(data.astype(np.float64) ** 2))) def detect_emotion(audio_path: str) -> dict:
"""
Read WAV at audio_path, compute spectral centroid and RMS, then:
- 'excited' if centroid>500 Hz and RMS>1000
- 'calm' if centroid<300 Hz and RMS<500
- otherwise 'neutral'
Returns dict with keys: 'emotion', 'spectral_centroid', 'rms'
"""
wf = wave.open(audio_path, "rb")
rate = wf.getframerate()
raw = wf.readframes(wf.getnframes())
wf.close()
kotlin
Copy
data = np.frombuffer(raw, dtype=np.int16)
centroid = compute_spectral_centroid(data, rate)
rms = compute_rms(data)

if centroid > 500.0 and rms > 1000.0:
    emotion = "excited"
elif centroid < 300.0 and rms < 500.0:
    emotion = "calm"
else:
    emotion = "neutral"

return {
    "emotion": emotion,
    "spectral_centroid": centroid,
    "rms": rms,
}
─────────────────────────────────────────────────────────────────────────────
File: tests/audio/test_emotion.py
References:
- ingest_bus.audio.emotion.detect_emotion
- numpy, wave (for sample WAV generation)
─────────────────────────────────────────────────────────────────────────────
import wave
import numpy as np
import pytest from tori.kha.ingest_bus.audio.emotion import detect_emotion def create_test_wav(path, freq=880, duration=0.5, rate=8000, amplitude=20000):
"""
Generate a WAV file with a sine wave:
x(t) = amplitude * sin(2π f t)
"""
t = np.linspace(0, duration, int(rate * duration), endpoint=False)
tone = (amplitude * np.sin(2 * np.pi * freq * t)).astype(np.int16)
with wave.open(path, "wb") as wf:
wf.setnchannels(1)
wf.setsampwidth(2)
wf.setframerate(rate)
wf.writeframes(tone.tobytes()) @pytest.fixture
def excited_wav(tmp_path):
file = tmp_path / "excited.wav"
# High freq (880 Hz) & high amplitude → should classify as 'excited'
create_test_wav(str(file), freq=880, amplitude=20000)
return str(file) @pytest.fixture
def calm_wav(tmp_path):
file = tmp_path / "calm.wav"
# Low freq (220 Hz) & low amplitude → should classify as 'calm'
create_test_wav(str(file), freq=220, amplitude=200)
return str(file) def test_detect_excited(excited_wav):
res = detect_emotion(excited_wav)
assert res["emotion"] == "excited"
assert isinstance(res["spectral_centroid"], float)
assert isinstance(res["rms"], float) def test_detect_calm(calm_wav):
res = detect_emotion(calm_wav)
assert res["emotion"] == "calm"
assert res["spectral_centroid"] < 300.0
assert res["rms"] < 500.0 def test_detect_neutral(tmp_path):
# Medium freq & amplitude → 'neutral'
file = tmp_path / "neutral.wav"
create_test_wav(str(file), freq=440, amplitude=8000)
res = detect_emotion(str(file))
assert res["emotion"] == "neutral"
# Centroid between thresholds
assert 300.0 <= res["spectral_centroid"] <= 500.0
# RMS between thresholds
assert 500.0 <= res["rms"] <= 1000.0
─────────────────────────────────────────────────────────────────────────────
File: ${IRIS_ROOT}\ingest_bus\video\ingest_video.py
References:
- cv2 (OpenCV) : frame sampling + color histogram
- numpy : array math
─────────────────────────────────────────────────────────────────────────────
"""
Ingest an MP4 / MOV: sample 1 fps, compute dominant hue & motion score,
return concept tags + hologram hints.
"""
import cv2
import numpy as np
from pathlib import Path
from dataclasses import dataclass
from typing import List @dataclass
class VideoIngestionResult:
concepts: List[str]
avg_hue: float # 0-179 OpenCV hue
motion_score: float # 0-1
holographic_hints: dict def ingest_video(path: str, sample_fps: int = 1) -> VideoIngestionResult:
cap = cv2.VideoCapture(str(path))
if not cap.isOpened():
raise RuntimeError(f"Cannot open {path}")
go
Copy
fps = cap.get(cv2.CAP_PROP_FPS) or 30
step = int(round(fps / sample_fps))
frames, hues, motions = [], [], []

ret, prev = cap.read()
frame_idx = 0
while ret:
    if frame_idx % step == 0:
        frames.append(prev)
        hsv = cv2.cvtColor(prev, cv2.COLOR_BGR2HSV)
        hue_hist = cv2.calcHist([hsv], [0], None, [180], [0, 180])
        dominant_hue = int(np.argmax(hue_hist))
        hues.append(dominant_hue)

    ret, curr = cap.read()
    if ret and frame_idx % step == 0:
        # Motion = mean abs diff grayscale
        motion = cv2.absdiff(cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY),
                             cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY))
        motions.append(float(np.mean(motion) / 255.0))
        prev = curr
    frame_idx += 1

cap.release()
if not frames:
    raise RuntimeError("No frames sampled")

avg_hue = float(np.mean(hues))
motion_score = float(np.mean(motions) if motions else 0.0)

# Simple concept tags
concepts = []
if avg_hue < 30 or avg_hue > 150:
    concepts.append("dominant_red_or_purple")
elif 30 <= avg_hue <= 90:
    concepts.append("dominant_green")
else:
    concepts.append("dominant_cyan")

if motion_score > 0.3:
    concepts.append("high_motion")

return VideoIngestionResult(
    concepts,
    avg_hue,
    motion_score,
    holographic_hints={
        "colorHue": avg_hue,
        "motionIntensity": motion_score,
    },
)
─────────────────────────────────────────────────────────────────────────────
File: ${IRIS_ROOT}\ingest_bus\video_init_.py
References:
– ingest_video.ingest_video
─────────────────────────────────────────────────────────────────────────────
from .ingest_video import ingest_video all = ["ingest_video"]
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\api\routes\video.py
References:
– fastapi.APIRouter
– ingest_bus.video.ingest_video
─────────────────────────────────────────────────────────────────────────────
from fastapi import APIRouter, UploadFile, File, HTTPException
import tempfile, os from tori.kha.ingest_bus.video import ingest_video router = APIRouter() @router.post("/video/ingest")
async def ingest_video_endpoint(file: UploadFile = File(...)):
_, ext = os.path.splitext(file.filename or "")
if ext.lower() not in {".mp4", ".mov", ".mkv", ".avi"}:
raise HTTPException(status_code=400, detail=f"Unsupported: {ext}")
python
Copy
tmp = tempfile.NamedTemporaryFile(delete=False, suffix=ext)
try:
    tmp.write(await file.read())
    tmp.flush()
    tmp.close()
    result = ingest_video(tmp.name)
except Exception as e:
    raise HTTPException(status_code=500, detail=str(e))
finally:
    try:
        os.unlink(tmp.name)
    except OSError:
        pass

return result.__dict__
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\api_init_.py (modify)
─────────────────────────────────────────────────────────────────────────────
from .routes import video
app.include_router(video.router, prefix="/api/v1", tags=["video"])
─────────────────────────────────────────────────────────────────────────────
File: tests/video/test_ingest_video.py
References:
– ingest_bus.video.ingest_video
– cv2, numpy (generate synthetic frames)
─────────────────────────────────────────────────────────────────────────────
import cv2, numpy as np, pytest, tempfile
from tori.kha.ingest_bus.video import ingest_video def create_dummy_mp4(path):
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(path, fourcc, 10, (64, 64))
for i in range(30):
frame = np.zeros((64, 64, 3), dtype=np.uint8)
if i % 2:
frame[:] = (0, 255, 0) # green frames → dominant_green
out.write(frame)
out.release() def test_ingest_video_green(tmp_path):
vid = tmp_path / "g.mp4"
create_dummy_mp4(str(vid))
res = ingest_video(str(vid))
assert "dominant_green" in res.concepts
assert 30 <= res.avg_hue <= 90
assert res.motion_score < 0.05
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\core\ghosts\trigger.js (NEW)
References:
- exports selectPersona({emotion, coherence, entropy})
- Persona IDs defined in ghostEngine.js
─────────────────────────────────────────────────────────────────────────────
/**
Simple rule-based selector that maps (emotion, coherence, entropy)
→ the persona name TORI should manifest.
emotion ∈ {calm, neutral, excited}
coherence ∈ [0,1] (semantic cohesion of conversation)
entropy ∈ [0,1] (conceptual chaos)
Rules (v1):
calm + high coherence → "Mentor"
excited + high entropy → "Chaotic"
neutral + low coherence → "Scholar"
excited + low coherence → "Unsettled" (user may be frantic)
default → "Default"
*/
export function selectPersona({ emotion, coherence, entropy }) {
if (emotion === "calm" && coherence > 0.7) return "Mentor";
if (emotion === "excited" && entropy > 0.6) return "Chaotic";
if (emotion === "neutral" && coherence < 0.4) return "Scholar";
if (emotion === "excited" && coherence < 0.4) return "Unsettled";
return "Default";
}
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\core\ghosts\ghostEngine.js (MODIFY)
References:
- ./trigger.selectPersona
- chat context (incoming user message with meta.audioEmotion…)
─────────────────────────────────────────────────────────────────────────────
import { selectPersona } from "./trigger.js"; /**
handleUserMessage(msg, stats)
– msg.text : user string
– msg.meta : { audioEmotion, spectralCentroid, rms }
– stats : { coherence, entropy } (computed upstream)
*/
export function handleUserMessage(msg, stats) {
const emotion = msg.meta?.audioEmotion || "neutral";
const { coherence = 0.5, entropy = 0.5 } = stats;
const persona = selectPersona({ emotion, coherence, entropy });
if (persona !== "Default") {
// Broadcast an internal event – UI overlay / avatar glow etc.
globalThis.dispatchEvent(
new CustomEvent("ghost:manifest", { detail: { persona } }),
);
} // proceed with normal assistant processing…
}
─────────────────────────────────────────────────────────────────────────────
File: tests/ghost/test_persona_select.js (NEW)
References:
- core/ghosts/trigger.selectPersona
─────────────────────────────────────────────────────────────────────────────
import { selectPersona } from "../../../core/ghosts/trigger.js";
import assert from "assert"; describe("selectPersona", () => {
it("picks Mentor for calm + coherent", () => {
assert.equal(
selectPersona({ emotion: "calm", coherence: 0.9, entropy: 0.2 }),
"Mentor",
);
}); it("picks Chaotic for excited + entropy", () => {
assert.equal(
selectPersona({ emotion: "excited", coherence: 0.8, entropy: 0.9 }),
"Chaotic",
);
}); it("defaults when no strong signal", () => {
assert.equal(
selectPersona({ emotion: "neutral", coherence: 0.6, entropy: 0.5 }),
"Default",
);
});
});
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\frontend\lib\audioRecorder.ts
References:
– browser MediaRecorder
– axios (HTTP POST to FastAPI)
– types/ChatMessage (inject user-side message)
─────────────────────────────────────────────────────────────────────────────
import axios from "axios"; /** Record mic, return a Promise<Blob> (WAV PCM-16-mono 48 kHz). */
export async function recordWav(seconds = 15): Promise<Blob> {
const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
const rec = new MediaRecorder(stream, { mimeType: "audio/webm" }); const chunks: BlobPart[] = [];
rec.ondataavailable = (e) => chunks.push(e.data); return new Promise<Blob>((resolve) => {
rec.onstop = () => {
const webmBlob = new Blob(chunks, { type: "audio/webm" });
resolve(webmBlob);
stream.getTracks().forEach((t) => t.stop());
};
rec.start();
setTimeout(() => rec.stop(), seconds * 1000);
});
} /** POST blob -> /api/v1/audio/ingest ; returns JSON response. */
export async function ingestAudio(blob: Blob) {
const form = new FormData();
form.append("file", blob, "voice.webm");
const { data } = await axios.post("/api/v1/audio/ingest", form, {
headers: { "Content-Type": "multipart/form-data" },
});
return data as {
transcript: string;
spectral_centroid: number;
rms: number;
emotion: string;
};
}
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\frontend\lib\holographicMemory.ts (NEW)
References:
Svelte/React stores – export { holoHints, activePersona }
─────────────────────────────────────────────────────────────────────────────
import { writable } from "svelte/store"; /** Latest hologram hints coming from video/audio ingestion. */
export const holoHints = writable<{
colorHue?: number; // 0-179 (OpenCV palette)
motionIntensity?: number; // 0-1
}>({}); /** Currently manifesting Ghost persona (set by Ghost overlay). */
export const activePersona = writable<string | null>(null);
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\frontend\context\ChatContext.tsx (modify)
References:
– adds meta param so we can pass emotion to ghosts later
─────────────────────────────────────────────────────────────────────────────
export interface ChatMessage {
role: "user" | "assistant" | "ghost";
text: string;
meta?: Record<string, any>; // ← new
} /* ... existing context code ... */ import { activePersona } from "../lib/holographicMemory"; // import store for ghost persona
// Assume context maintains persona state
const [persona, setPersona] = useState<string | null>(null); useEffect(() => {
const onManifest = (e: any) => {
const p = e.detail.persona;
setPersona(p);
activePersona.set(p);
};
window.addEventListener("ghost:manifest", onManifest);
return () => window.removeEventListener("ghost:manifest", onManifest);
}, []); /* rest of context unchanged */
─────────────────────────────────────────────────────────────────────────────
File: C:\Users\jason\Desktop\tori\frontend\components\HologramCanvas.tsx (NEW)
References:
– three (npm install three @react-three/fiber)
– recoil / zustand / context not needed – we pull from holographicMemory store
─────────────────────────────────────────────────────────────────────────────
import React, { useRef, useEffect } from "react";
import { Canvas, useFrame } from "@react-three/fiber";
import * as THREE from "three";
import { holoHints, activePersona } from "../lib/holographicMemory";
import { useStore } from "react-svelte-store"; // thin helper – install function Orb() {
const mesh = useRef<THREE.Mesh>(null);
// Subscribe to Svelte store inside React:
const hints = useStore(holoHints);
const persona = useStore(activePersona); // Map hue to THREE.Color
const color = new THREE.Color();
color.setHSL(((hints.colorHue ?? 180) / 360), 0.7, 0.5); useFrame((state, dt) => {
// Pulse scale by motionIntensity
const m = hints.motionIntensity ?? 0;
const scale = 1 + m * Math.sin(state.clock.elapsedTime * 4);
if (mesh.current) mesh.current.scale.set(scale, scale, scale);
csharp
Copy
// Subtle glow: if a ghost persona is active, modulate emissive
if (mesh.current) {
  const mat = mesh.current.material as THREE.MeshStandardMaterial;
  mat.emissiveIntensity = persona ? 1.0 : 0.1;
}
}); return (
<mesh ref={mesh}>
<sphereGeometry args={[1, 32, 32]} />
<meshStandardMaterial color={color} emissive={color} emissiveIntensity={0.1} />
</mesh>
);
} export default function HologramCanvas() {
return (
<div style={{ width: "240px", height: "240px" }}>
<Canvas camera={{ position: [0, 0, 3] }}>
<ambientLight intensity={0.6} />
<pointLight position={[3, 3, 3]} />
<Orb />
</Canvas>
</div>
);
}
─────────────────────────────────────────────────────────────────────────────
<!-- File: C:\Users\jason\Desktop\tori\frontend\ui/HolographicVisualization.svelte References: lib/holographicMemory store ──────────────────────────────────────────────────────────────────────────────--> <script lang="ts"> import { onMount } from "svelte"; import { holoHints, activePersona } from "../lib/holographicMemory"; let canvas: HTMLCanvasElement; let gl: WebGLRenderingContext; let hue = 180, motion = 0, glow = 0; const unsub1 = holoHints.subscribe(h => { hue = h.colorHue ?? 180; motion = h.motionIntensity ?? 0; }); const unsub2 = activePersona.subscribe(p => glow = p ? 1 : 0); onMount(() => { gl = canvas.getContext("2d") as any; const loop = () => { const t = performance.now() / 1000; const r = Math.sin(t * 4) * motion * 30 + 50; gl.fillStyle = `hsl(${hue},70%,${glow?60:50}%)`; gl.clearRect(0,0,240,240); gl.beginPath(); gl.arc(120,120,r,0,Math.PI*2); gl.fill(); requestAnimationFrame(loop); }; loop(); return () => { unsub1(); unsub2(); }; }); </script> <canvas bind:this={canvas} width="240" height="240" class="holo"></canvas> <style> .holo { border-radius: 8px; background: #111; } </style> /* frontend/styles/quick-bar.css */
.voice-btn.recording {
color: red;
animation: pulse 1s infinite;
}
@keyframes pulse {
50% { transform: scale(1.2); }
} /* frontend/styles/ghost-overlay.css */
.ghost-overlay {
position: absolute;
bottom: 10%;
right: 2%;
padding: 6px 12px;
border-radius: 8px;
font-weight: 600;
color: #fff;
pointer-events: none;
}