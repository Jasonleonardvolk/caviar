With the six key areas covered, we now present a step-by-step implementation plan that follows this blueprint.
Step-by-Step Implementation Plan
To implement ELFIN’s evolution from concept to reality, we outline the following technical steps:
Define the Language Specification: Start by writing a draft specification document. Include the lexical rules (all tokens and examples of valid/invalid identifiers, literals, etc.) and the grammar (in BNF or EBNF) for the core language (expressions, statements, functions, modules). This spec will serve as a reference for implementation and allow early feedback. Use the current .elfin scripts to validate that the grammar can express them. For example, ensure the grammar allows matrix literals as used in matrix.elfin and function calls or control structures as used in pid.elfin. Iterate on the grammar until it’s unambiguous and covers all desired features. Having a formal grammar also enables using parser generators.
Implement the Lexer: Using the lexical rules, implement a lexer that can tokenize ELFIN source code. If using a parser generator like ANTLR, you can write lexer rules in the grammar file. If implementing in Python, consider using PLY (Python Lex-Yacc) or regex to match tokens. The lexer should recognize keywords (e.g. fn, import, etc.), identifiers, numbers, string literals, and symbols (operators, punctuation). Test the lexer independently – feed it with sample code and ensure the sequence of tokens is as expected. For instance, tokenizing A = [1,2,3]; should produce an identifier A, an equals, a left bracket, numbers, commas, right bracket, semicolon. Include recognition of indentation or line breaks if those are syntactically significant (if we choose Python-like significant indentation, the lexer needs to generate INDENT/DEDENT tokens; if not, we can ignore whitespace and use ; or newlines to terminate statements). At this step, also build a simple error-handling strategy: the lexer should report or at least throw an understandable error if it encounters an illegal character or an unterminated string, etc.
Implement the Parser: With tokens flowing, implement a parser that builds an Abstract Syntax Tree (AST). If using ANTLR, write the grammar in .g4 file and generate a parser in Python (or Java, etc.). If using Python libraries, Lark is very convenient: you can provide an EBNF grammar and get an AST or parse tree out of the box. Alternatively, one can write a recursive descent parser by hand (possibly using the Pratt parsing technique for expressions). The parser should construct AST node objects for each significant construct (e.g. ProgramNode containing a list of statements, AssignNode for assignments, BinOpNode for binary operations, etc.). Design the AST classes or data structures; this is important for the next stage. At this point, focus on correctness – the AST should accurately represent the code’s structure. Write unit tests with small snippets (and the example scripts) to ensure the parser produces the expected AST shape. For example, parse x = 5 + 3*2 and check that the AST is something like AssignNode(var='x', value=BinOpNode(left=Number(5), op='+', right=BinOpNode(left=Number(3), op='*', right=Number(2)) ) ). This step may surface ambiguities or conflicts in the grammar (especially if you have left recursion or operator precedence issues); resolve those by adjusting grammar or using parser generator features (like precedence declarations in Lark or ANTLR). Ensure that the parser can recover or at least report errors nicely when given incomplete/mistyped code, as this will feed into editor diagnostics.
Semantic Analysis and Type Checking: Once an AST is built, perform semantic analysis. This includes building symbol tables for variable/function definitions, scope handling, and enforcing any static rules of the language. For example, check for undefined variables, duplicate definitions, or type mismatches if using static typing. If we decided on a gradual type system, implement the basics of it here: e.g., if a variable is declared with a type, store that in the symbol table and later when that variable is used, ensure type consistency (or insert runtime type checks for dynamic parts). We might implement a simple type inferencer or just explicit checkers. Also, handle things like ensuring a return statement is inside a function, break is inside a loop, etc. This phase can also annotate the AST with type information or other metadata needed for code generation. Use the example scripts to test semantic rules – e.g., if linear.elfin defines a matrix and uses it in arithmetic, ensure the type system (if any) can handle that (maybe treat matrix operations as valid if both sides are matrices). Since initially the type system may be mostly dynamic, a lot of this may be just checking for things like arity of function calls, making sure imported modules exist, etc. This is also a good place to desugar any syntactic sugar: e.g., transform a for x in range(0,10) loop into a form of a while loop or an equivalent AST that’s easier for the interpreter.
Build the Initial Interpreter: Implement an interpreter that walks the AST and executes it. This could be a straightforward recursive interpreter in Python: define an eval(node, env) function that pattern-matches on node type and executes accordingly. For example, evaluating an AssignNode will evaluate the expression on the right, then store it in the current environment (which could be a Python dict representing variables). Evaluating a BinOpNode will eval the left and right, then apply the operator. Implement control flow by managing an instruction pointer or using recursion: for a IfNode, eval the condition, then depending on true/false, eval the appropriate branch. For loops, use a Python while or for loop to repeatedly eval the body. You’ll need to handle function calls: perhaps create a call stack of environments so that each function call gets its own local scope (environment), with access to outer scope for closures as needed. Closures can be handled by storing a reference to the defining environment inside function AST nodes or a callable object. Also integrate built-in functions (like print or any TORI-provided functions): these can be special cases in the interpreter environment – e.g. have a dictionary of built-in names to Python functions that implement them. At this stage, focus on getting the core working. Test by running simple programs in the interpreter and comparing results with expectation. Use the example .elfin scripts: run them in the interpreter and see if they produce the expected outcome (for instance, if linear.elfin computes some matrix result, verify the interpreter gets it right, or if pid.elfin prints control values over time, check the sequence). This will validate not only the interpreter but everything from lexer to semantics.
Error Handling & Debug Info: Improve the interpreter to handle runtime errors gracefully. For example, if there’s a division by zero or an index out of bounds in a vector, catch it and throw an ELFIN runtime error (possibly with traceback). Use exceptions in the implementation language (Python) to propagate errors up, and catch them to format a nice message with the ELFIN source line (we can track source locations in AST nodes from the parser). This step is important for usability – a cryptic error will confuse users, so we want messages like “RuntimeError: Division by zero in function foo at line 12”. Also, ensure the interpreter can be interrupted (for REPL or long-running loops), maybe by handling a keyboard interrupt. Now is a good time to implement a basic REPL loop using the interpreter: read input, feed to parser, eval AST, print result (if expression) or nothing (if statement). This REPL will aid further manual testing and will be the basis for the user-facing REPL.
Implement the Compiler/Transpiler (as needed): With a working interpreter, decide on the next execution mode. For instance, implement a bytecode compiler: write a pass that takes the AST and emits a sequence of bytecode instructions. Design a bytecode format (e.g., simple instructions like LOAD_CONST, LOAD_VAR, ADD, CALL, JUMP_IF_FALSE, etc.). Then implement a bytecode virtual machine to run these. This can increase speed compared to AST walking because bytecode is more linear and cache-friendly. Test the bytecode engine with the same programs and compare results. Alternatively, implement a transpiler to Python at this stage: traverse the AST and generate equivalent Python code (perhaps as text or using the ast module). Then exec that Python code and use Python’s execution for results. Validate that for various inputs, the transpiled code yields correct results. If going for WebAssembly or JS next, you might skip Python transpilation for now and instead try a small C code generation or WASM generation for arithmetic parts to gauge difficulty. Possibly start with a very limited subset to compile (like arithmetic expressions to WASM) and embed that into the interpreter for heavy loops (like a JIT for math). Choose based on priorities – if TORI integration (Python) is most important, do Python transpiler first; if performance in browser is key, do WASM.
Develop the Standard Library and Integration Hooks: Begin implementing standard library functions and modules. For math, include basic linear algebra (if not relying on Python’s numpy through interop). For example, write ELFIN (or host-language) code for matrix multiplication, inversion, etc., or bind to existing ones (like call NumPy if in Python, or a C library if in C). Provide modules for common tasks (maybe a robot module with kinematics functions, a control module with a ready-made PID controller class, etc., possibly derived from the .elfin scripts). Simultaneously, add the integration points to Python: e.g., ensure you can call Python functions from ELFIN. This could mean in the interpreter, when a foreign function call node is encountered, use Python’s importlib to get the function and call it. Or provide a special syntax or built-in like py("some.python.expr"). Test integration by calling a simple Python function (like math.sin) from an ELFIN snippet. If that works, try something from TORI (if available as Python), like if TORI has a function raise_concept("fear"), attempt to call it. This step likely requires close knowledge of the TORI API, so involve TORI developers to expose what needs to be accessible.
Build Developer Tools (Iteratively): Start with easier ones:
Syntax Highlighting: Create a VSCode extension (even a basic one) with a TextMate grammar derived from your EBNF. This can be done by hand or using tools. Test it on example scripts to see that keywords, numbers, strings get colored. Publish or share it for team to use when writing ELFIN.
LSP Server Skeleton: Use a framework like pygls (if in Python) to create a language server. Implement initialize, and at least a basic textDocument/didOpen handler that invokes the parser on the file and reports syntax errors as LSP diagnostics. This immediately gives users real-time feedback on mistakes. Then implement completion: when the editor asks for completions, return a static list for now or simple ones like all variables in scope (you can maintain scope by reusing the semantic analysis from the compiler). Continue to add features: hover (show type or documentation on hover), goto definition (you have symbol tables from semantic analysis, so map variable/function name to its definition location), etc. This will evolve as the language and compiler do. By doing it early, you ensure the compiler exposes the info needed (like it might prompt you to store line numbers, doc comments, type info in a symbol table accessible to the LSP).
Debugging Tools: Implement a simple debug mode in the interpreter: e.g., a special command to step through code. You can integrate this with the REPL (like a :debug command that runs the next user command in step mode). Once that’s working, create a Debug Adapter for VSCode: basically a small program (could be in Python or Node) that uses the DAP protocol. It will launch the ELFIN interpreter, set breakpoints (you’ll need to instruct the interpreter to check for breakpoints by matching file and line), and communicate events back to VSCode. Alternatively, since implementing a full DAP is complex, you could defer this and rely on the REPL + print debugging initially. However, given the nature of robotics, a proper debugging where you can pause the system is extremely valuable (to examine sensor values, etc.). Perhaps by the time reactive features are added, a debugger will be essential to manage complexity.
Package Management Setup: Even if we don’t have a full package manager yet, decide on a directory structure and perhaps create a template repository (for example on GitHub) for an ELFIN project. This can include a README, a folder for .elfin files, and maybe a script to run them. Over time, one might write a simple tool (in Python or as part of the elfin CLI) to install packages from Git or a registry. We won’t implement a whole registry initially, but perhaps have a centralized Git or file share for internal TORI use where people can share .elfin libraries.
Continuous Testing: Set up a test suite and maybe continuous integration (CI) to run all parser tests, interpreter tests, etc., on each change. This will catch regressions as we add new features (e.g., adding reactive constructs shouldn’t break basic arithmetic). Testing the tooling is also important – e.g., ensure the LSP’s completions are correct via an automated test if possible (some LSP testing frameworks exist, or just script the LSP with known inputs).
Introduce Concurrency/Parallelism: Before tackling the advanced ψ constructs, implement a basic concurrency mechanism if planned (to draw from Elixir’s actor model). For instance, allow spawning a new OS thread or async task to run an ELFIN function. In Python, this could tie into asyncio or threading. In a future lower-level implementation, it could be native threads or an event loop. Provide a simple API like spawn(func, args) and perhaps message passing like send(task, message) and a receive() block to handle messages. This groundwork will be useful when adding reactive signals (which often conceptually run in parallel to the main code, updating as time progresses). Test concurrency with simple examples (like spawn two tasks that print or increment counters). Make sure to handle synchronization or at least document that concurrent access to variables needs locks or is not allowed (depending on the model, maybe each actor has isolated memory like Erlang – which is safer).
Implement Reactive Features (Signals and Rules): Now, using the groundwork, implement the reactive system:
Introduce a Signal type in the runtime. It might hold a current value and a list of dependents (callbacks or other signals that derive from it). Implement operations on signals: e.g., if you do signal c = a + b where a and b are signals, then c becomes a signal that updates whenever a or b changes. This might involve overriding the evaluation of + when its operands are signals to produce a new signal.
Implement the syntax support: the parser should recognize a signal declaration and create appropriate AST nodes. The interpreter should create Signal objects for these and hook them into an event loop. You might implement a simple scheduler: e.g., at end of each evaluation cycle (or every time an input signal is updated), recompute all derived signals. Alternatively, push updates immediately (but that can cause cascading updates; a tick-based approach might be easier to reason about in a first implementation).
Implement the on (condition) { ... } or similar reactive rule construct. Parsing it likely produces a node that contains the condition expression and the body. In the interpreter, you could set up a callback that evaluates the condition whenever any signal in it changes from false to true. Possibly reuse the signal mechanism by treating the condition as a signal (boolean signal), and then the rule triggers on the rising edge of that boolean becoming true. This may require storing some state (to know the previous value).
Provide a time or tick mechanism if needed: e.g., a built-in tick() function to advance simulation time, or integrate with real time (calls to sleep or using real clock).
Test with scenarios: e.g., create a signal that is a sine wave (maybe update it in a loop) and a rule that triggers when it exceeds a threshold. Ensure the rule fires correctly and only when expected.
Optimize or refine: possibly use a library like RxPY (ReactiveX for Python) to manage signals, if it fits – it could save time to reuse known patterns.
Add Cognitive Constructs (Concepts, Constraints): Work on formalizing and implementing concept anchors and constraints:
Decide on syntax: e.g., concept X = expr for anchors, and maybe a constraint { ... } block for cognitive constraints, or a keyword like ensure condition; that must always hold.
Parser: add these constructs. For constraints, you might parse a constraint block into a list of boolean expressions representing each constraint.
Runtime: for concept anchors, perhaps maintain a global mapping of concept names to values, and tie it to the cognitive model (if TORI provides an API, use it: for example, whenever a concept is assigned in ELFIN, call tori.set_concept(name, value)). Conversely, if the cognitive model changes a concept (e.g., through some reasoning process), the ELFIN signal or variable should update – this is effectively another reactive source, so concept anchors can be implemented as signals under the hood. That way, if a concept changes, any rules depending on it fire. If concepts are more static, at least ensure the ELFIN side always reflects the true state (maybe on each cycle, sync from TORI).
For constraints, implement checking: simplest is to evaluate the constraint conditions at certain points (like end of each tick or whenever relevant variables change) and if any are false, raise an alert or try to enforce them. Enforcement could mean rollback a change or adjust a variable (which enters control theory territory). Possibly at first just log a warning or error if a constraint is violated. Later, one could integrate a solver: for example, if a constraint is A + B == 10 and the user sets A=7, the runtime could automatically set B=3 to satisfy it. This is constraint solving and can be complex; a simpler approach is to treat constraints as assertions (not to be violated).
Formalize psi usage: If ψ is used to mark something (like ψX in code), implement that. It might just be syntactic sugar for concept anchor of name X, or some kind of macro. Define exactly how it works and test it with expected domain scenarios (maybe TORI team can provide a sample logic formula involving ψ that we can try to represent in ELFIN).
Example integration: Suppose cognitive theory says “if concept Fear’s activation goes above 0.8, concept Panic triggers after 5 seconds”. In ELFIN, one might write:
elfin
Copy
concept Fear = 0.0
concept Panic = false

on (Fear > 0.8) {
    delay(5.0);   # wait 5 seconds (we'd need a delay function)
    Panic = true;
}
This shows combining concept anchors (Fear, Panic) with a reactive rule and temporal delay. To support this, we implement a delay() in the rule that schedules the remainder of the block later. Essentially, our runtime’s event loop would need to handle timed events (which is doable using a priority queue of scheduled actions).
This step is where we align the language closely with the domain. Involve domain experts heavily, adjust syntax if needed to be natural for them (maybe they prefer a rule syntax like IF <condition> FOR <duration> THEN <action> etc. – be open to that if it reads more like their domain language).
Document these new features clearly, as they are not typical programming constructs. Provide examples and specify their execution model (perhaps in an appendix to the spec focusing on reactive/cognitive extensions).
Optimization and Testing: As features accumulate, revisit performance. Profile the interpreter on heavy tasks (maybe a large matrix multiplication or a tight control loop with many signals). If too slow, consider optimizations: e.g., implement critical operations in C (via Python C extensions or Rust if possible), or improve the bytecode compiler. Also test memory usage (especially with signals – ensure we aren’t leaking subscriptions or creating too many objects). Write more tests, including integration tests where a piece of code uses multiple features together (e.g., a module that defines a concept and a reactive rule and a function, to ensure all interplay works). Testing should also cover error conditions, concurrency issues (race conditions if any), etc. Since robotics can be safety-critical, try some fault injection (like simulate a null sensor reading and see if the program handles it or at least fails gracefully).
Release Documentation and Examples: Prepare comprehensive documentation for users. This includes a language reference (with the final grammar, list of all keywords, built-in functions, etc.), a tutorial (perhaps walking through rewriting the original .elfin scripts in the new language step by step), and reference of the standard library. Provide the example scripts as part of examples folder, possibly with added commentary. If possible, create a small demo project (maybe a robot simulation in a browser using ELFIN code, or a console app that runs a cognitive model) to showcase the language. This will not only help users but also validate that the language is truly useful in practice.
Community and Feedback: Once an initial version is ready, involve the early adopters (TORI developers, robotics researchers) to try writing new scripts in ELFIN. Gather feedback on syntax (is anything awkward?), performance (is it fast enough for their loops?), missing features, and bugs. Use this to iterate on the design. Perhaps some adjustments will be made (e.g. if a particular cognitive construct isn’t expressive enough, we might refine its syntax or semantics). This feedback loop will drive future improvements and ensure the language is grounded in real needs.
Throughout these steps, we leverage open-source components: parsing libraries (ANTLR, Lark, PLY), LSP frameworks (like Microsoft’s LSP libraries), and possibly LLVM or existing compilers for backend, as well as scientific libraries (NumPy, etc.) for heavy computation. By not reinventing wheels, we can focus on the unique parts of ELFIN. We also maintain the preservation of citations and references in any documentation we create (like if we include external definitions, we’d cite them as we did in this plan). In conclusion, this plan provides a roadmap to evolve ELFIN from a narrow DSL into a full programming language with modern features, strong tooling, and domain-specific power. By following these steps and incorporating best practices from language design
matt.might.net
, along with inspirations from Lua’s embeddability
sciencedirect.com
, Elixir’s concurrency
elixirschool.com
, Scheme’s minimalism
docs.scheme.org
, and more, ELFIN will grow into a robust tool for robotics and cognitive system development. Each phase builds upon the previous, ensuring that at every stage we have a working system that can be tested against real use-cases. The end result will be a language that not only meets the immediate needs (linear algebra, control, robotics scripting) but is extensible into advanced cognitive programming, all while being accessible and efficient for developers.