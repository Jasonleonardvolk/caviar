# 🎯 **BPS INTEGRATION TEST SUITE RUNNER** 🚀\n# ═══════════════════════════════════════════════════════════════════════════════\n# Master test runner for the complete BPS ecosystem integration test suite\n# Orchestrates all test categories with comprehensive reporting\n# ═══════════════════════════════════════════════════════════════════════════════\n\nimport pytest\nimport sys\nimport time\nimport logging\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler('bps_integration_tests.log')\n    ]\n)\nlogger = logging.getLogger('BPS_Integration_Runner')\n\n# Import BPS config for reporting\ntry:\n    from python.core.bps_config import *\n    BPS_CONFIG_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"BPS config not available for test runner\")\n    BPS_CONFIG_AVAILABLE = False\n    # Set fallback values\n    STRICT_BPS_MODE = False\n    ENABLE_BPS_DIAGNOSTICS = True\n    PERFORMANCE_PROFILING_ENABLED = False\n\n# ═══════════════════════════════════════════════════════════════════════════════\n# TEST SUITE CONFIGURATION\n# ═══════════════════════════════════════════════════════════════════════════════\n\nclass TestSuiteConfig:\n    \"\"\"Configuration for the test suite execution\"\"\"\n    \n    def __init__(self):\n        self.test_categories = {\n            'integration': {\n                'file': 'test_bps_integration.py',\n                'description': 'Core BPS ecosystem integration tests',\n                'priority': 1,\n                'estimated_time': '2-5 minutes',\n                'required': True\n            },\n            'conservation': {\n                'file': 'test_conservation_laws.py', \n                'description': 'Conservation law validation tests',\n                'priority': 1,\n                'estimated_time': '1-3 minutes',\n                'required': True\n            },\n            'performance': {\n                'file': 'test_performance_benchmarks.py',\n                'description': 'Performance and scaling benchmarks', \n                'priority': 2,\n                'estimated_time': '3-10 minutes',\n                'required': False\n            }\n        }\n        \n        self.pytest_args = {\n            'verbose': ['-v'],\n            'detailed': ['-v', '-s'],\n            'fast': ['-x', '--tb=short'],\n            'comprehensive': ['-v', '-s', '--tb=long'],\n            'performance': ['-v', '-s', '--tb=short', '-k', 'performance']\n        }\n\nclass TestResults:\n    \"\"\"Container for test execution results\"\"\"\n    \n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n        self.category_results = {}\n        self.overall_status = 'unknown'\n        self.total_tests = 0\n        self.passed_tests = 0\n        self.failed_tests = 0\n        self.skipped_tests = 0\n        self.errors = []\n        self.warnings = []\n    \n    def add_category_result(self, category: str, result: Dict[str, Any]):\n        \"\"\"Add results for a test category\"\"\"\n        self.category_results[category] = result\n        \n        # Update totals\n        if 'total' in result:\n            self.total_tests += result['total']\n        if 'passed' in result:\n            self.passed_tests += result['passed']\n        if 'failed' in result:\n            self.failed_tests += result['failed']\n        if 'skipped' in result:\n            self.skipped_tests += result['skipped']\n    \n    def finalize(self):\n        \"\"\"Finalize results and determine overall status\"\"\"\n        self.end_time = time.time()\n        \n        if self.failed_tests == 0 and self.total_tests > 0:\n            self.overall_status = 'passed'\n        elif self.failed_tests > 0:\n            self.overall_status = 'failed'\n        elif self.total_tests == 0:\n            self.overall_status = 'no_tests'\n        else:\n            self.overall_status = 'unknown'\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive test summary\"\"\"\n        duration = (self.end_time - self.start_time) if self.start_time and self.end_time else 0\n        \n        return {\n            'timestamp': datetime.now().isoformat(),\n            'duration_seconds': duration,\n            'overall_status': self.overall_status,\n            'total_tests': self.total_tests,\n            'passed_tests': self.passed_tests,\n            'failed_tests': self.failed_tests,\n            'skipped_tests': self.skipped_tests,\n            'pass_rate': (self.passed_tests / max(self.total_tests, 1)) * 100,\n            'categories_tested': len(self.category_results),\n            'category_results': self.category_results,\n            'errors': self.errors,\n            'warnings': self.warnings,\n            'bps_config_available': BPS_CONFIG_AVAILABLE,\n            'strict_mode': STRICT_BPS_MODE if BPS_CONFIG_AVAILABLE else False,\n            'diagnostics_enabled': ENABLE_BPS_DIAGNOSTICS if BPS_CONFIG_AVAILABLE else False\n        }\n\n# ═══════════════════════════════════════════════════════════════════════════════\n# TEST SUITE RUNNER\n# ═══════════════════════════════════════════════════════════════════════════════\n\nclass BPSIntegrationTestRunner:\n    \"\"\"Master test runner for BPS integration tests\"\"\"\n    \n    def __init__(self, config: TestSuiteConfig):\n        self.config = config\n        self.results = TestResults()\n        self.test_dir = Path(__file__).parent\n    \n    def run_category(self, category: str, pytest_args: List[str] = None) -> Dict[str, Any]:\n        \"\"\"Run tests for a specific category\"\"\"\n        category_info = self.config.test_categories[category]\n        test_file = self.test_dir / category_info['file']\n        \n        if not test_file.exists():\n            error_msg = f\"Test file not found: {test_file}\"\n            logger.error(error_msg)\n            return {\n                'status': 'error',\n                'error': error_msg,\n                'total': 0,\n                'passed': 0,\n                'failed': 1,\n                'skipped': 0\n            }\n        \n        logger.info(f\"🧪 Running {category} tests: {category_info['description']}\")\n        logger.info(f\"📁 File: {test_file.name}\")\n        logger.info(f\"⏱️  Estimated time: {category_info['estimated_time']}\")\n        \n        # Prepare pytest arguments\n        if pytest_args is None:\n            pytest_args = self.config.pytest_args['detailed']\n        \n        full_args = [str(test_file)] + pytest_args\n        \n        try:\n            start_time = time.time()\n            \n            # Run pytest\n            result = pytest.main(full_args)\n            \n            end_time = time.time()\n            duration = end_time - start_time\n            \n            # Parse results based on exit code\n            category_result = self._parse_pytest_results(result, duration)\n            category_result['category'] = category\n            category_result['description'] = category_info['description']\n            \n            logger.info(f\"✅ {category} tests completed in {duration:.1f}s\")\n            logger.info(f\"📊 Status: {category_result['status']}\")\n            \n            return category_result\n            \n        except Exception as e:\n            error_msg = f\"Failed to run {category} tests: {e}\"\n            logger.error(error_msg)\n            return {\n                'status': 'error',\n                'error': error_msg,\n                'category': category,\n                'total': 0,\n                'passed': 0,\n                'failed': 1,\n                'skipped': 0\n            }\n    \n    def _parse_pytest_results(self, exit_code: int, duration: float) -> Dict[str, Any]:\n        \"\"\"Parse pytest results from exit code\"\"\"\n        return {\n            'status': 'passed' if exit_code == 0 else 'failed',\n            'duration': duration,\n            'total': 1,\n            'passed': 1 if exit_code == 0 else 0,\n            'failed': 0 if exit_code == 0 else 1,\n            'skipped': 0,\n            'exit_code': exit_code\n        }\n    \n    def run_all_tests(self, mode: str = 'comprehensive', categories: List[str] = None) -> TestResults:\n        \"\"\"Run all or specified test categories\"\"\"\n        logger.info(\"🚀 BPS INTEGRATION TEST SUITE STARTING!\")\n        logger.info(f\"🎯 Mode: {mode}\")\n        logger.info(f\"⚙️  BPS Config: {'AVAILABLE' if BPS_CONFIG_AVAILABLE else 'FALLBACK'}\")\n        logger.info(f\"🔒 Strict Mode: {'ON' if STRICT_BPS_MODE else 'OFF'}\")\n        \n        self.results.start_time = time.time()\n        \n        # Determine which categories to run\n        if categories is None:\n            if mode == 'fast':\n                categories = [cat for cat, info in self.config.test_categories.items() if info['required']]\n            else:\n                categories = list(self.config.test_categories.keys())\n        \n        # Sort by priority\n        categories.sort(key=lambda cat: self.config.test_categories[cat]['priority'])\n        \n        logger.info(f\"📋 Test categories to run: {', '.join(categories)}\")\n        \n        # Get pytest arguments for mode\n        pytest_args = self.config.pytest_args.get(mode, self.config.pytest_args['detailed'])\n        \n        # Run each category\n        for i, category in enumerate(categories, 1):\n            logger.info(f\"\\n{'='*60}\")\n            logger.info(f\"🧪 CATEGORY {i}/{len(categories)}: {category.upper()}\")\n            logger.info(f\"{'='*60}\")\n            \n            try:\n                category_result = self.run_category(category, pytest_args)\n                self.results.add_category_result(category, category_result)\n                \n                # Check if we should continue\n                if category_result['status'] == 'failed' and mode == 'fast':\n                    logger.warning(f\"⏹️  Stopping due to failure in {category} (fast mode)\")\n                    break\n                    \n            except KeyboardInterrupt:\n                logger.warning(\"⏹️  Test execution interrupted by user\")\n                self.results.errors.append(\"Test execution interrupted\")\n                break\n            except Exception as e:\n                error_msg = f\"Unexpected error in {category}: {e}\"\n                logger.error(error_msg)\n                self.results.errors.append(error_msg)\n        \n        # Finalize results\n        self.results.finalize()\n        \n        # Print summary\n        self._print_final_summary()\n        \n        return self.results\n    \n    def _print_final_summary(self):\n        \"\"\"Print final test summary\"\"\"\n        summary = self.results.get_summary()\n        \n        logger.info(f\"\\n{'='*80}\")\n        logger.info(\"🎯 BPS INTEGRATION TEST SUITE FINAL SUMMARY\")\n        logger.info(f\"{'='*80}\")\n        \n        # Overall status\n        status_emoji = {\n            'passed': '✅',\n            'failed': '❌', \n            'no_tests': '⚠️',\n            'unknown': '❓'\n        }\n        \n        logger.info(f\"📊 Overall Status: {status_emoji.get(summary['overall_status'], '❓')} {summary['overall_status'].upper()}\")\n        logger.info(f\"⏱️  Total Duration: {summary['duration_seconds']:.1f} seconds\")\n        logger.info(f\"🧪 Total Tests: {summary['total_tests']}\")\n        logger.info(f\"✅ Passed: {summary['passed_tests']}\")\n        logger.info(f\"❌ Failed: {summary['failed_tests']}\")\n        logger.info(f\"⏭️  Skipped: {summary['skipped_tests']}\")\n        logger.info(f\"📈 Pass Rate: {summary['pass_rate']:.1f}%\")\n        \n        # Category breakdown\n        logger.info(f\"\\n📋 Category Results:\")\n        for category, result in summary['category_results'].items():\n            status = result.get('status', 'unknown')\n            emoji = status_emoji.get(status, '❓')\n            passed = result.get('passed', 0)\n            total = result.get('total', 0)\n            duration = result.get('duration', 0)\n            \n            logger.info(f\"  {emoji} {category}: {passed}/{total} passed ({duration:.1f}s)\")\n        \n        # Configuration info\n        logger.info(f\"\\n⚙️  Configuration:\")\n        logger.info(f\"  • BPS Config: {'Available' if summary['bps_config_available'] else 'Fallback'}\")\n        logger.info(f\"  • Strict Mode: {'Enabled' if summary['strict_mode'] else 'Disabled'}\")\n        logger.info(f\"  • Diagnostics: {'Enabled' if summary['diagnostics_enabled'] else 'Disabled'}\")\n        \n        logger.info(f\"\\n{'='*80}\")\n        \n        if summary['overall_status'] == 'passed':\n            logger.info(\"🎉 ALL TESTS PASSED! BPS ecosystem is ready for production! 🚀\")\n        elif summary['overall_status'] == 'failed':\n            logger.info(\"💥 SOME TESTS FAILED! Review failures before production deployment.\")\n        else:\n            logger.info(\"⚠️  TEST RESULTS INCONCLUSIVE! Manual review required.\")\n        \n        logger.info(f\"{'='*80}\")\n\n# ═══════════════════════════════════════════════════════════════════════════════\n# MAIN ENTRY POINT\n# ═══════════════════════════════════════════════════════════════════════════════\n\ndef main():\n    \"\"\"Main entry point for the test suite runner\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description=\"BPS Integration Test Suite Runner\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python run_integration_tests.py                    # Run all tests (comprehensive mode)\n  python run_integration_tests.py --mode fast        # Run required tests only\n  python run_integration_tests.py --categories integration conservation  # Run specific categories\n  python run_integration_tests.py --mode performance # Run performance tests only\n\nTest Categories:\n  integration    - Core BPS ecosystem integration tests\n  conservation   - Conservation law validation tests\n  performance    - Performance and scaling benchmarks\n        \"\"\"\n    )\n    \n    parser.add_argument(\n        '--mode',\n        choices=['fast', 'comprehensive', 'performance', 'verbose'],\n        default='comprehensive',\n        help='Test execution mode (default: comprehensive)'\n    )\n    \n    parser.add_argument(\n        '--categories',\n        nargs='+',\n        choices=['integration', 'conservation', 'performance'],\n        help='Specific test categories to run (default: all)'\n    )\n    \n    parser.add_argument(\n        '--output',\n        choices=['console', 'json', 'both'],\n        default='console',\n        help='Output format (default: console)'\n    )\n    \n    parser.add_argument(\n        '--save-results',\n        metavar='FILE',\n        help='Save detailed results to JSON file'\n    )\n    \n    parser.add_argument(\n        '--verbose',\n        action='store_true',\n        help='Enable verbose output'\n    )\n    \n    args = parser.parse_args()\n    \n    # Configure logging level\n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    # Create and run test suite\n    config = TestSuiteConfig()\n    runner = BPSIntegrationTestRunner(config)\n    \n    # Run tests\n    results = runner.run_all_tests(\n        mode=args.mode,\n        categories=args.categories\n    )\n    \n    # Save results if requested\n    if args.save_results:\n        summary = results.get_summary()\n        with open(args.save_results, 'w') as f:\n            json.dump(summary, f, indent=2)\n        logger.info(f\"📄 Results saved to: {args.save_results}\")\n    \n    # Output JSON if requested\n    if args.output in ['json', 'both']:\n        print(\"\\n\" + \"=\"*60)\n        print(\"JSON RESULTS:\")\n        print(\"=\"*60)\n        print(json.dumps(results.get_summary(), indent=2))\n    \n    # Exit with appropriate code\n    exit_code = 0 if results.overall_status == 'passed' else 1\n    sys.exit(exit_code)\n\nif __name__ == \"__main__\":\n    main()\n"