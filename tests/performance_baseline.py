#!/usr/bin/env python3\n\"\"\"Performance Baseline Script for TORI System\"\"\"\n\nimport asyncio\nimport json\nimport time\nfrom pathlib import Path\nimport statistics\n\nimport httpx\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.pagesizes import letter\n\nclass TORIPerformanceBaseline:\n    def __init__(self, base_url=\"http://localhost:8002\"):\n        self.base_url = base_url\n        self.results = {\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"upload_performance\": {},\n            \"chat_performance\": {},\n            \"system_performance\": {},\n            \"overall_score\": 0\n        }\n    \n    def generate_test_pdf(self, content: str, filename: str) -> str:\n        \"\"\"Generate a test PDF for performance testing\"\"\"\n        import tempfile\n        import os\n        \n        temp_dir = tempfile.gettempdir()\n        pdf_path = os.path.join(temp_dir, filename)\n        \n        c = canvas.Canvas(pdf_path, pagesize=letter)\n        width, height = letter\n        \n        c.setFont(\"Helvetica-Bold\", 16)\n        c.drawString(50, height - 50, \"Performance Test Document\")\n        \n        c.setFont(\"Helvetica\", 12)\n        y_position = height - 100\n        \n        lines = content.split('\\n')\n        for line in lines:\n            if y_position < 50:\n                c.showPage()\n                y_position = height - 50\n            c.drawString(50, y_position, line)\n            y_position -= 20\n        \n        c.save()\n        return pdf_path\n    \n    async def test_upload_performance(self, iterations=5):\n        \"\"\"Test upload performance with multiple iterations\"\"\"\n        print(\"🔍 Testing upload performance...\")\n        \n        content = \"\"\"Performance Test Document\n        \nThis document is used for performance testing of the TORI system.\nIt contains sample content about artificial intelligence and machine learning.\n\nKey concepts:\n- Neural networks\n- Deep learning\n- Natural language processing\n- Computer vision\n- Reinforcement learning\n\nThe purpose is to measure:\n- Upload speed\n- Processing time\n- Concept extraction efficiency\n- SSE streaming performance\n\"\"\"\n        \n        pdf_path = self.generate_test_pdf(content, \"performance_test.pdf\")\n        upload_times = []\n        concept_counts = []\n        \n        async with httpx.AsyncClient(base_url=self.base_url, timeout=60.0) as client:\n            for i in range(iterations):\n                start_time = time.time()\n                progress_id = f\"perf_test_{i}_{int(time.time())}\"\n                \n                try:\n                    with open(pdf_path, \"rb\") as pdf_file:\n                        files = {\"file\": (f\"perf_test_{i}.pdf\", pdf_file, \"application/pdf\")}\n                        response = await client.post(\n                            f\"/api/upload?progress_id={progress_id}\",\n                            files=files\n                        )\n                    \n                    if response.status_code == 200:\n                        data = response.json()\n                        duration = time.time() - start_time\n                        upload_times.append(duration)\n                        concept_counts.append(data.get(\"document\", {}).get(\"concept_count\", 0))\n                        print(f\"  Upload {i+1}: {duration:.2f}s, {concept_counts[-1]} concepts\")\n                    else:\n                        print(f\"  Upload {i+1}: Failed ({response.status_code})\")\n                        \n                except Exception as e:\n                    print(f\"  Upload {i+1}: Error - {e}\")\n                \n                await asyncio.sleep(1)  # Brief pause between uploads\n        \n        if upload_times:\n            self.results[\"upload_performance\"] = {\n                \"iterations\": len(upload_times),\n                \"avg_time\": statistics.mean(upload_times),\n                \"min_time\": min(upload_times),\n                \"max_time\": max(upload_times),\n                \"std_dev\": statistics.stdev(upload_times) if len(upload_times) > 1 else 0,\n                \"avg_concepts\": statistics.mean(concept_counts) if concept_counts else 0,\n                \"success_rate\": len(upload_times) / iterations * 100\n            }\n    \n    async def test_chat_performance(self, iterations=10):\n        \"\"\"Test chat response performance\"\"\"\n        print(\"💬 Testing chat performance...\")\n        \n        queries = [\n            \"What is artificial intelligence?\",\n            \"Explain machine learning algorithms\",\n            \"How do neural networks work?\",\n            \"What are the applications of deep learning?\",\n            \"Describe natural language processing\"\n        ]\n        \n        chat_times = []\n        response_lengths = []\n        confidence_scores = []\n        \n        async with httpx.AsyncClient(base_url=self.base_url, timeout=30.0) as client:\n            for i in range(iterations):\n                query = queries[i % len(queries)]\n                start_time = time.time()\n                \n                try:\n                    response = await client.post(\n                        \"/api/answer\",\n                        json={\n                            \"user_query\": f\"{query} (Test {i+1})\",\n                            \"persona\": {\"name\": \"PerformanceTester\"}\n                        }\n                    )\n                    \n                    if response.status_code == 200:\n                        data = response.json()\n                        duration = time.time() - start_time\n                        chat_times.append(duration)\n                        response_lengths.append(len(data.get(\"answer\", \"\")))\n                        confidence_scores.append(data.get(\"confidence\", 0))\n                        print(f\"  Chat {i+1}: {duration:.2f}s, {response_lengths[-1]} chars, confidence: {confidence_scores[-1]:.2f}\")\n                    else:\n                        print(f\"  Chat {i+1}: Failed ({response.status_code})\")\n                        \n                except Exception as e:\n                    print(f\"  Chat {i+1}: Error - {e}\")\n                \n                await asyncio.sleep(0.5)\n        \n        if chat_times:\n            self.results[\"chat_performance\"] = {\n                \"iterations\": len(chat_times),\n                \"avg_time\": statistics.mean(chat_times),\n                \"min_time\": min(chat_times),\n                \"max_time\": max(chat_times),\n                \"std_dev\": statistics.stdev(chat_times) if len(chat_times) > 1 else 0,\n                \"avg_response_length\": statistics.mean(response_lengths) if response_lengths else 0,\n                \"avg_confidence\": statistics.mean(confidence_scores) if confidence_scores else 0,\n                \"success_rate\": len(chat_times) / iterations * 100\n            }\n    \n    async def test_system_endpoints(self):\n        \"\"\"Test system monitoring endpoints\"\"\"\n        print(\"🔧 Testing system endpoints...\")\n        \n        endpoints = {\n            \"health\": \"/api/health\",\n            \"validate\": \"/api/system/validate\",\n            \"performance\": \"/api/system/performance\",\n            \"debug\": \"/api/system/debug\"\n        }\n        \n        endpoint_times = {}\n        \n        async with httpx.AsyncClient(base_url=self.base_url, timeout=15.0) as client:\n            for name, endpoint in endpoints.items():\n                start_time = time.time()\n                try:\n                    response = await client.get(endpoint)\n                    duration = time.time() - start_time\n                    \n                    if response.status_code == 200:\n                        endpoint_times[name] = duration\n                        print(f\"  {name}: {duration:.3f}s\")\n                    else:\n                        print(f\"  {name}: Failed ({response.status_code})\")\n                        \n                except Exception as e:\n                    print(f\"  {name}: Error - {e}\")\n        \n        self.results[\"system_performance\"] = {\n            \"endpoint_times\": endpoint_times,\n            \"avg_endpoint_time\": statistics.mean(endpoint_times.values()) if endpoint_times else 0,\n            \"endpoints_tested\": len(endpoints),\n            \"endpoints_successful\": len(endpoint_times)\n        }\n    \n    def calculate_overall_score(self):\n        \"\"\"Calculate overall performance score (0-100)\"\"\"\n        score = 100\n        \n        # Upload performance scoring\n        upload_perf = self.results.get(\"upload_performance\", {})\n        if upload_perf:\n            avg_upload_time = upload_perf.get(\"avg_time\", 0)\n            upload_success = upload_perf.get(\"success_rate\", 0)\n            \n            # Deduct points for slow uploads (target: <10s)\n            if avg_upload_time > 10:\n                score -= min(20, (avg_upload_time - 10) * 2)\n            \n            # Deduct points for low success rate\n            if upload_success < 100:\n                score -= (100 - upload_success) / 2\n        \n        # Chat performance scoring\n        chat_perf = self.results.get(\"chat_performance\", {})\n        if chat_perf:\n            avg_chat_time = chat_perf.get(\"avg_time\", 0)\n            chat_success = chat_perf.get(\"success_rate\", 0)\n            avg_confidence = chat_perf.get(\"avg_confidence\", 0)\n            \n            # Deduct points for slow responses (target: <3s)\n            if avg_chat_time > 3:\n                score -= min(15, (avg_chat_time - 3) * 3)\n            \n            # Deduct points for low success rate\n            if chat_success < 100:\n                score -= (100 - chat_success) / 2\n            \n            # Deduct points for low confidence\n            if avg_confidence < 0.7:\n                score -= (0.7 - avg_confidence) * 20\n        \n        # System endpoint scoring\n        system_perf = self.results.get(\"system_performance\", {})\n        if system_perf:\n            endpoint_success_rate = (system_perf.get(\"endpoints_successful\", 0) / \n                                   system_perf.get(\"endpoints_tested\", 1)) * 100\n            if endpoint_success_rate < 100:\n                score -= (100 - endpoint_success_rate) / 4\n        \n        self.results[\"overall_score\"] = max(0, score)\n    \n    def print_summary(self):\n        \"\"\"Print performance summary\"\"\"\n        print(\"\\n📊 PERFORMANCE BASELINE RESULTS\")\n        print(\"=\" * 50)\n        \n        # Upload Performance\n        upload = self.results.get(\"upload_performance\", {})\n        if upload:\n            print(f\"\\n📤 Upload Performance:\")\n            print(f\"  Average Time: {upload.get('avg_time', 0):.2f}s\")\n            print(f\"  Success Rate: {upload.get('success_rate', 0):.1f}%\")\n            print(f\"  Avg Concepts: {upload.get('avg_concepts', 0):.1f}\")\n            print(f\"  Time Range: {upload.get('min_time', 0):.2f}s - {upload.get('max_time', 0):.2f}s\")\n        \n        # Chat Performance\n        chat = self.results.get(\"chat_performance\", {})\n        if chat:\n            print(f\"\\n💬 Chat Performance:\")\n            print(f\"  Average Time: {chat.get('avg_time', 0):.2f}s\")\n            print(f\"  Success Rate: {chat.get('success_rate', 0):.1f}%\")\n            print(f\"  Avg Confidence: {chat.get('avg_confidence', 0):.2f}\")\n            print(f\"  Avg Response Length: {chat.get('avg_response_length', 0):.0f} chars\")\n        \n        # System Performance\n        system = self.results.get(\"system_performance\", {})\n        if system:\n            print(f\"\\n🔧 System Performance:\")\n            print(f\"  Endpoint Success: {system.get('endpoints_successful', 0)}/{system.get('endpoints_tested', 0)}\")\n            print(f\"  Avg Response Time: {system.get('avg_endpoint_time', 0):.3f}s\")\n        \n        # Overall Score\n        score = self.results[\"overall_score\"]\n        print(f\"\\n🎯 Overall Performance Score: {score:.1f}/100\")\n        \n        if score >= 90:\n            print(\"🎉 EXCELLENT - Ready for production deployment!\")\n        elif score >= 75:\n            print(\"✅ GOOD - Minor optimizations recommended\")\n        elif score >= 60:\n            print(\"⚠️ FAIR - Performance improvements needed\")\n        else:\n            print(\"❌ POOR - Significant optimization required\")\n    \n    def save_results(self, filename=\"performance_baseline.json\"):\n        \"\"\"Save results to JSON file\"\"\"\n        with open(filename, \"w\") as f:\n            json.dump(self.results, f, indent=2)\n        print(f\"\\n💾 Results saved to {filename}\")\n\nasync def main():\n    print(\"🚀 TORI Performance Baseline Test\")\n    print(\"=\" * 40)\n    \n    baseline = TORIPerformanceBaseline()\n    \n    # Check API availability\n    try:\n        async with httpx.AsyncClient(base_url=\"http://localhost:8002\", timeout=10.0) as client:\n            response = await client.get(\"/api/health\")\n            if response.status_code != 200:\n                print(\"❌ API not available. Start with: python enhanced_launcher.py\")\n                return\n    except Exception as e:\n        print(f\"❌ API connection failed: {e}\")\n        return\n    \n    start_time = time.time()\n    \n    # Run performance tests\n    await baseline.test_upload_performance(iterations=3)\n    await baseline.test_chat_performance(iterations=8)\n    await baseline.test_system_endpoints()\n    \n    # Calculate results\n    baseline.calculate_overall_score()\n    \n    total_time = time.time() - start_time\n    print(f\"\\n⏱️ Total test time: {total_time:.1f}s\")\n    \n    # Display and save results\n    baseline.print_summary()\n    baseline.save_results()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n