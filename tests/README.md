# ðŸ§ª TORI Automated End-to-End Testing Suite\n\n**Comprehensive testing framework for bulletproof production deployment confidence**\n\n## ðŸŽ¯ Overview\n\nThis testing suite validates the complete TORI document intelligence pipeline:\n- **Upload Pipeline**: PDF upload with SSE progress tracking\n- **Document Processing**: Concept extraction and ConceptMesh integration  \n- **Chat Intelligence**: Document-grounded responses with source attribution\n- **System Performance**: Concurrent load testing and validation\n- **Integration**: Complete uploadâ†’processâ†’chatâ†’validate workflow\n\n## ðŸš€ Quick Start\n\n### 1. Install Dependencies\n```bash\ncd tests\npip install -r requirements.txt\n```\n\n### 2. Start TORI API\n```bash\ncd ..\npython enhanced_launcher.py\n```\n\n### 3. Run Tests\n```bash\n# Quick test suite (recommended for CI)\npython run_tests.py --quick\n\n# Full test suite including stress tests\npython run_tests.py\n\n# Integration test only\npython run_tests.py --integration\n```\n\n## ðŸ“Š Test Categories\n\n### ðŸ”§ Upload Tests (`--upload`)\n- **PDF Upload with SSE Progress**: Validates real-time progress tracking\n- **File Validation**: Tests file type restrictions and error handling\n- **Progress Events**: Verifies all expected SSE stages (startingâ†’validatingâ†’savingâ†’processingâ†’complete)\n- **Concept Extraction**: Confirms document processing and concept count\n\n### ðŸ’¬ Chat Tests (`--chat`)\n- **Document-Grounded Responses**: Questions answered using uploaded document content\n- **Source Attribution**: Verifies responses cite specific documents\n- **Context Validation**: Confirms `context_used: \"document_grounded\"`\n- **Keyword Matching**: Validates answers contain expected domain-specific terms\n- **Performance**: Response time under 10 seconds\n\n### ðŸ§ª Stress Tests (`--stress`)\n- **Concurrent Operations**: 50 parallel upload + chat workers\n- **Success Rate Validation**: â‰¥90% upload success, â‰¥95% chat success\n- **Performance Thresholds**: <15s avg upload, <5s avg chat\n- **Resource Management**: Memory and connection pressure testing\n\n### ðŸ”— Integration Tests (`--integration`)\n- **Complete Pipeline**: Uploadâ†’ConceptMeshâ†’Chatâ†’Validation in single workflow\n- **End-to-End Timing**: Full pipeline under 30 seconds\n- **System Health**: Validates monitoring endpoints\n- **Real-world Workflow**: Simulates actual user experience\n\n## ðŸ“‹ Test Configuration\n\n### Default Settings (pytest.ini)\n```ini\n[tool:pytest]\naddopts = -v --tb=short --asyncio-mode=auto\nmarkers =\n    e2e: End-to-end integration tests\n    stress: Load and stress tests  \n    upload: PDF upload tests\n    chat: Chat/answer tests\n    slow: Tests that take longer than 10 seconds\n```\n\n### Environment Configuration\n```python\nTEST_CONFIG = {\n    \"api_base_url\": \"http://localhost:8002\",\n    \"timeout\": 30.0,\n    \"sse_timeout\": 60.0, \n    \"stress_test_workers\": 50,\n    \"expected_response_time\": 2.0\n}\n```\n\n## ðŸ§° Test Utilities\n\n### SSE Progress Tracker\n```python\nclass SSEProgressTracker:\n    async def track_progress(self, timeout=60.0) -> Dict[str, Any]:\n        # Tracks SSE events until completion\n        # Returns: {\"success\": bool, \"events\": list, \"duration\": float}\n```\n\n### Test PDF Generation\n```python\n@pytest.fixture\ndef test_pdf_quantum():\n    # Generates PDF with quantum physics content\n    # Known content for testing document-grounded responses\n\n@pytest.fixture  \ndef test_pdf_ml():\n    # Generates PDF with machine learning content\n    # Used for stress testing and multi-document scenarios\n```\n\n### HTTP Client\n```python\n@pytest.fixture\nasync def http_client():\n    # Async HTTP client with proper timeout and base URL\n    # Automatically validates API health before tests\n```\n\n## ðŸ“Š Success Criteria\n\n### Upload Pipeline\n- âœ… **Upload Success**: 200 response with valid document JSON\n- âœ… **SSE Progress**: All expected stages received\n- âœ… **Concept Extraction**: Non-zero concept count\n- âœ… **Performance**: Upload completion within 15 seconds\n\n### Chat Intelligence  \n- âœ… **Document Grounding**: `context_used: \"document_grounded\"`\n- âœ… **Source Attribution**: Document titles in `sources` array\n- âœ… **Content Quality**: Answer length >50 chars, relevant keywords\n- âœ… **Performance**: Response generation within 10 seconds\n\n### System Performance\n- âœ… **Stress Test**: â‰¥90% upload success rate under load\n- âœ… **Concurrent Chat**: â‰¥95% chat success rate under load \n- âœ… **Latency**: <15s avg upload, <5s avg chat response\n- âœ… **Integration**: Complete pipeline within 30 seconds\n\n## ðŸ” Test Execution Examples\n\n### Development Testing\n```bash\n# Quick validation during development\npython run_tests.py --quick --verbose\n\n# Focus on upload functionality\npython run_tests.py --upload\n\n# Test chat responses only\npython run_tests.py --chat\n```\n\n### CI/CD Pipeline\n```bash\n# Comprehensive test suite for deployment\npython run_tests.py --verbose\n\n# Performance validation\npython run_tests.py --stress\n\n# Critical path validation\npython run_tests.py --integration\n```\n\n### Manual Test Execution\n```bash\n# Individual test files\npytest test_e2e.py::TestEndToEndPipeline::test_pdf_upload_with_sse_progress -v\n\n# Specific test markers\npytest -m \"upload and not slow\" -v\n\n# With custom configuration\npytest --tb=long -s test_e2e.py\n```\n\n## ðŸ“ˆ Performance Monitoring\n\n### Real-time Metrics\n- **Upload Duration**: Time from POST to SSE completion\n- **SSE Event Count**: Number of progress updates received\n- **Chat Response Time**: Query to answer generation\n- **Document Consultation**: Number of sources referenced\n- **Concurrent Load**: Success rates under parallel execution\n\n### Validation Endpoints\n- `/api/system/validate` - Overall system health\n- `/api/system/performance` - Real-time performance metrics\n- `/api/system/stress-test` - Load capacity simulation\n- `/api/health` - Basic service availability\n\n## ðŸ› ï¸ Debugging\n\n### Test Failures\n```bash\n# Verbose output with stack traces\npython run_tests.py --verbose\n\n# Skip health check if API issues\npython run_tests.py --no-health-check\n\n# Run single test for debugging\npytest test_e2e.py::test_complete_pipeline_integration -v -s\n```\n\n### Common Issues\n- **API Not Available**: Ensure `enhanced_launcher.py` is running\n- **SSE Timeout**: Check network connectivity and server load\n- **Upload Failures**: Verify PDF file permissions and disk space\n- **Chat Failures**: Confirm ConceptMesh has uploaded documents\n\n## ðŸŽ¯ Production Readiness\n\n### Deployment Checklist\n- âœ… All E2E tests passing\n- âœ… Stress test success rates â‰¥90%\n- âœ… Performance thresholds met\n- âœ… Integration workflow validated\n- âœ… System monitoring endpoints operational\n\n### CI Integration\n```yaml\n# Example GitHub Actions workflow\nname: TORI E2E Tests\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n      - name: Install dependencies\n        run: |\n          cd tests\n          pip install -r requirements.txt\n      - name: Start TORI API\n        run: python enhanced_launcher.py &\n      - name: Wait for API\n        run: sleep 10\n      - name: Run E2E Tests\n        run: python tests/run_tests.py --quick\n```\n\n## ðŸš€ Next Steps\n\nAfter successful test validation:\n\n1. **ðŸŒŸ Staging Deployment**: Deploy to staging environment\n2. **ðŸ‘¥ User Acceptance Testing**: Real-world user workflows\n3. **ðŸ“Š Performance Optimization**: Based on test metrics\n4. **ðŸ”„ Continuous Integration**: Automated testing pipeline\n5. **ðŸŽ¯ Production Deployment**: With complete confidence\n\n**The automated testing suite ensures TORI is bulletproof and ready for production deployment!** ðŸŽ‰\n