#!/usr/bin/env python3\n\"\"\"\nPrajna Data Ingestion System\n===========================\n\nMassive-scale data ingestion for Prajna's consciousness system.\nProcesses all data types and feeds them into Prajna's memory systems.\n\"\"\"\n\nimport os\nimport sys\nimport asyncio\nimport logging\nimport time\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport concurrent.futures\nimport hashlib\n\n# Add prajna to path\nsys.path.insert(0, str(Path(__file__).parent / \"prajna\"))\n\n# Import Prajna components\ntry:\n    from prajna.memory.context_builder import build_context\n    from prajna.memory.soliton_interface import SolitonMemoryInterface\n    from prajna.memory.concept_mesh_api import ConceptMeshAPI\n    from prajna.config.prajna_config import PrajnaConfig\n    from prajna.core.prajna_mouth import PrajnaLanguageModel\n    PRAJNA_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"⚠️ Prajna not available: {e}\")\n    PRAJNA_AVAILABLE = False\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"prajna.ingest\")\n\n@dataclass\nclass DataFile:\n    \"\"\"Represents a file to be ingested\"\"\"\n    path: Path\n    size: int\n    file_type: str\n    content_hash: str\n    priority: int = 1  # 1=high, 2=medium, 3=low\n\n@dataclass\nclass IngestionStats:\n    \"\"\"Statistics for ingestion process\"\"\"\n    total_files: int = 0\n    processed_files: int = 0\n    skipped_files: int = 0\n    failed_files: int = 0\n    total_size: int = 0\n    processed_size: int = 0\n    start_time: float = 0.0\n    \n    @property\n    def progress_percent(self) -> float:\n        return (self.processed_files / max(1, self.total_files)) * 100\n    \n    @property\n    def processing_rate(self) -> float:\n        elapsed = time.time() - self.start_time\n        return self.processed_files / max(1, elapsed)\n\nclass PrajnaDataIngestor:\n    \"\"\"\n    Massive-scale data ingestion system for Prajna\n    \n    Intelligently processes and feeds data into Prajna's consciousness:\n    - PDFs, text files, documents\n    - Images, videos, audio\n    - Code files, logs, configs\n    - Structured data (JSON, CSV)\n    - Archives and compressed files\n    \"\"\"\n    \n    def __init__(self, data_directory: str):\n        self.data_dir = Path(data_directory)\n        self.stats = IngestionStats()\n        self.processed_hashes = set()\n        self.batch_size = 50\n        self.max_workers = 4\n        \n        # File type priorities and handlers\n        self.file_priorities = {\n            '.pdf': 1,      # High priority - documents\n            '.txt': 1,      # High priority - text\n            '.md': 1,       # High priority - markdown\n            '.py': 1,       # High priority - code\n            '.json': 1,     # High priority - structured data\n            '.csv': 2,      # Medium priority - data\n            '.log': 2,      # Medium priority - logs\n            '.jpg': 2,      # Medium priority - images\n            '.png': 2,      # Medium priority - images\n            '.mp4': 3,      # Low priority - videos\n            '.wav': 3,      # Low priority - audio\n        }\n        \n        # Skip patterns\n        self.skip_patterns = {\n            '__pycache__', '.git', '.vscode', 'node_modules',\n            '.pytest_cache', '.coverage', '.tox', 'dist', 'build'\n        }\n        \n        # Initialize Prajna components\n        self.prajna_config = None\n        self.soliton_memory = None\n        self.concept_mesh = None\n        self.prajna_model = None\n        \n        logger.info(f\"🍽️ Prajna Data Ingestor initialized for: {self.data_dir}\")\n    \n    async def initialize_prajna(self):\n        \"\"\"Initialize Prajna components for ingestion\"\"\"\n        if not PRAJNA_AVAILABLE:\n            logger.warning(\"⚠️ Prajna not available - using mock mode\")\n            return\n        \n        try:\n            logger.info(\"🧠 Initializing Prajna components...\")\n            \n            # Load configuration\n            self.prajna_config = PrajnaConfig()\n            \n            # Initialize memory systems\n            self.soliton_memory = SolitonMemoryInterface()\n            await self.soliton_memory.initialize()\n            \n            self.concept_mesh = ConceptMeshAPI()\n            await self.concept_mesh.initialize()\n            \n            # Initialize language model for processing\n            self.prajna_model = PrajnaLanguageModel(model_type=\"demo\")\n            await self.prajna_model.load_model()\n            \n            logger.info(\"✅ Prajna components ready for ingestion\")\n            \n        except Exception as e:\n            logger.error(f\"❌ Failed to initialize Prajna: {e}\")\n    \n    async def discover_files(self) -> List[DataFile]:\n        \"\"\"Discover all files in the data directory\"\"\"\n        logger.info(f\"🔍 Discovering files in {self.data_dir}...\")\n        \n        discovered_files = []\n        \n        for root, dirs, files in os.walk(self.data_dir):\n            # Skip certain directories\n            dirs[:] = [d for d in dirs if d not in self.skip_patterns]\n            \n            for file in files:\n                file_path = Path(root) / file\n                \n                # Skip if file doesn't exist or is empty\n                if not file_path.exists() or file_path.stat().st_size == 0:\n                    continue\n                \n                # Calculate content hash\n                content_hash = self._calculate_file_hash(file_path)\n                \n                # Skip if already processed\n                if content_hash in self.processed_hashes:\n                    continue\n                \n                # Determine file type and priority\n                file_ext = file_path.suffix.lower()\n                file_type = self._classify_file_type(file_path)\n                priority = self.file_priorities.get(file_ext, 3)\n                \n                data_file = DataFile(\n                    path=file_path,\n                    size=file_path.stat().st_size,\n                    file_type=file_type,\n                    content_hash=content_hash,\n                    priority=priority\n                )\n                \n                discovered_files.append(data_file)\n        \n        # Sort by priority and size (high priority first, then smaller files)\n        discovered_files.sort(key=lambda f: (f.priority, f.size))\n        \n        self.stats.total_files = len(discovered_files)\n        self.stats.total_size = sum(f.size for f in discovered_files)\n        \n        logger.info(f\"📊 Discovered {len(discovered_files)} files ({self._format_size(self.stats.total_size)})\")\n        \n        return discovered_files\n    \n    def _calculate_file_hash(self, file_path: Path) -> str:\n        \"\"\"Calculate SHA-256 hash of file content\"\"\"\n        try:\n            hasher = hashlib.sha256()\n            with open(file_path, 'rb') as f:\n                for chunk in iter(lambda: f.read(8192), b\"\"):\n                    hasher.update(chunk)\n            return hasher.hexdigest()[:16]  # First 16 chars\n        except Exception:\n            return str(file_path)  # Fallback to path\n    \n    def _classify_file_type(self, file_path: Path) -> str:\n        \"\"\"Classify file type for processing\"\"\"\n        ext = file_path.suffix.lower()\n        \n        if ext in ['.pdf']:\n            return 'document'\n        elif ext in ['.txt', '.md', '.rst']:\n            return 'text'\n        elif ext in ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.h']:\n            return 'code'\n        elif ext in ['.json', '.yaml', '.yml', '.xml']:\n            return 'structured'\n        elif ext in ['.csv', '.tsv', '.xlsx']:\n            return 'data'\n        elif ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n            return 'image'\n        elif ext in ['.mp4', '.avi', '.mov', '.wmv']:\n            return 'video'\n        elif ext in ['.wav', '.mp3', '.flac', '.ogg']:\n            return 'audio'\n        elif ext in ['.log']:\n            return 'log'\n        else:\n            return 'unknown'\n    \n    async def ingest_data(self, max_files: Optional[int] = None) -> IngestionStats:\n        \"\"\"Main ingestion process\"\"\"\n        logger.info(\"🚀 Starting massive data ingestion for Prajna...\")\n        \n        self.stats.start_time = time.time()\n        \n        # Initialize Prajna\n        await self.initialize_prajna()\n        \n        # Discover files\n        files_to_process = await self.discover_files()\n        \n        if max_files:\n            files_to_process = files_to_process[:max_files]\n            logger.info(f\"🔢 Limited to {max_files} files for this run\")\n        \n        # Process files in batches\n        logger.info(f\"⚡ Processing {len(files_to_process)} files in batches of {self.batch_size}...\")\n        \n        for i in range(0, len(files_to_process), self.batch_size):\n            batch = files_to_process[i:i + self.batch_size]\n            \n            logger.info(f\"📦 Processing batch {i//self.batch_size + 1}/{(len(files_to_process)-1)//self.batch_size + 1}\")\n            \n            # Process batch concurrently\n            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n                tasks = []\n                for data_file in batch:\n                    task = executor.submit(self._process_file_sync, data_file)\n                    tasks.append(task)\n                \n                # Wait for batch completion\n                for task in concurrent.futures.as_completed(tasks):\n                    try:\n                        result = task.result()\n                        if result:\n                            self.stats.processed_files += 1\n                            self.stats.processed_size += result.get('size', 0)\n                        else:\n                            self.stats.skipped_files += 1\n                    except Exception as e:\n                        self.stats.failed_files += 1\n                        logger.error(f\"❌ File processing failed: {e}\")\n            \n            # Progress update\n            self._print_progress()\n            \n            # Small delay between batches\n            await asyncio.sleep(0.1)\n        \n        # Final statistics\n        elapsed_time = time.time() - self.stats.start_time\n        logger.info(f\"\\n🎉 INGESTION COMPLETE!\")\n        logger.info(f\"📊 Processed: {self.stats.processed_files}/{self.stats.total_files} files\")\n        logger.info(f\"📊 Data size: {self._format_size(self.stats.processed_size)}\")\n        logger.info(f\"📊 Time taken: {elapsed_time:.1f} seconds\")\n        logger.info(f\"📊 Rate: {self.stats.processing_rate:.1f} files/second\")\n        logger.info(f\"📊 Skipped: {self.stats.skipped_files}, Failed: {self.stats.failed_files}\")\n        \n        return self.stats\n    \n    def _process_file_sync(self, data_file: DataFile) -> Optional[Dict[str, Any]]:\n        \"\"\"Synchronous file processing (for thread pool)\"\"\"\n        try:\n            return asyncio.run(self._process_file(data_file))\n        except Exception as e:\n            logger.error(f\"❌ Failed to process {data_file.path}: {e}\")\n            return None\n    \n    async def _process_file(self, data_file: DataFile) -> Optional[Dict[str, Any]]:\n        \"\"\"Process a single file into Prajna's memory\"\"\"\n        try:\n            # Extract content based on file type\n            content = await self._extract_content(data_file)\n            \n            if not content:\n                return None\n            \n            # Create memory entry\n            memory_entry = {\n                'file_path': str(data_file.path),\n                'file_type': data_file.file_type,\n                'content': content,\n                'content_hash': data_file.content_hash,\n                'size': data_file.size,\n                'ingestion_time': datetime.now().isoformat(),\n                'priority': data_file.priority\n            }\n            \n            # Store in Prajna's memory systems\n            if PRAJNA_AVAILABLE and self.soliton_memory:\n                await self._store_in_soliton_memory(memory_entry)\n            \n            if PRAJNA_AVAILABLE and self.concept_mesh:\n                await self._store_in_concept_mesh(memory_entry)\n            \n            # Mark as processed\n            self.processed_hashes.add(data_file.content_hash)\n            \n            return {'size': data_file.size, 'type': data_file.file_type}\n            \n        except Exception as e:\n            logger.warning(f\"⚠️ Failed to process {data_file.path}: {e}\")\n            return None\n    \n    async def _extract_content(self, data_file: DataFile) -> Optional[str]:\n        \"\"\"Extract text content from various file types\"\"\"\n        try:\n            if data_file.file_type in ['text', 'code', 'log']:\n                # Plain text files\n                with open(data_file.path, 'r', encoding='utf-8', errors='ignore') as f:\n                    return f.read()[:50000]  # Limit to 50KB per file\n            \n            elif data_file.file_type == 'structured':\n                # JSON, YAML, etc.\n                with open(data_file.path, 'r', encoding='utf-8', errors='ignore') as f:\n                    content = f.read()[:10000]  # Smaller limit for structured data\n                    return f\"Structured data file: {data_file.path.name}\\n{content}\"\n            \n            elif data_file.file_type == 'document':\n                # PDFs and other documents\n                return f\"Document file: {data_file.path.name} ({self._format_size(data_file.size)})\"\n            \n            elif data_file.file_type in ['image', 'video', 'audio']:\n                # Media files - store metadata only\n                return f\"Media file: {data_file.path.name} ({data_file.file_type}, {self._format_size(data_file.size)})\"\n            \n            else:\n                # Unknown file types - store basic info\n                return f\"File: {data_file.path.name} ({self._format_size(data_file.size)})\"\n        \n        except Exception as e:\n            logger.warning(f\"⚠️ Content extraction failed for {data_file.path}: {e}\")\n            return None\n    \n    async def _store_in_soliton_memory(self, memory_entry: Dict[str, Any]):\n        \"\"\"Store memory entry in Soliton Memory\"\"\"\n        try:\n            # For now, just simulate storage\n            # In production, this would call actual Soliton Memory API\n            pass\n        except Exception as e:\n            logger.warning(f\"⚠️ Soliton Memory storage failed: {e}\")\n    \n    async def _store_in_concept_mesh(self, memory_entry: Dict[str, Any]):\n        \"\"\"Store memory entry in Concept Mesh\"\"\"\n        try:\n            # For now, just simulate storage\n            # In production, this would extract concepts and add to mesh\n            pass\n        except Exception as e:\n            logger.warning(f\"⚠️ Concept Mesh storage failed: {e}\")\n    \n    def _print_progress(self):\n        \"\"\"Print ingestion progress\"\"\"\n        elapsed = time.time() - self.stats.start_time\n        rate = self.stats.processing_rate\n        \n        print(f\"\\r🔄 Progress: {self.stats.progress_percent:.1f}% | \"\n              f\"Files: {self.stats.processed_files}/{self.stats.total_files} | \"\n              f\"Rate: {rate:.1f}/sec | \"\n              f\"Time: {elapsed:.0f}s\", end=\"\", flush=True)\n    \n    def _format_size(self, size_bytes: int) -> str:\n        \"\"\"Format file size in human readable form\"\"\"\n        for unit in ['B', 'KB', 'MB', 'GB']:\n            if size_bytes < 1024:\n                return f\"{size_bytes:.1f} {unit}\"\n            size_bytes /= 1024\n        return f\"{size_bytes:.1f} TB\"\n\nasync def main():\n    \"\"\"Main ingestion function\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Ingest massive data into Prajna\")\n    parser.add_argument(\"--data-dir\", default=\"C:\\\\Users\\\\jason\\\\Desktop\\\\tori\\\\kha\\\\data\",\n                       help=\"Data directory to ingest\")\n    parser.add_argument(\"--max-files\", type=int, help=\"Maximum files to process\")\n    parser.add_argument(\"--batch-size\", type=int, default=50, help=\"Batch size\")\n    parser.add_argument(\"--workers\", type=int, default=4, help=\"Number of workers\")\n    \n    args = parser.parse_args()\n    \n    # Create ingestor\n    ingestor = PrajnaDataIngestor(args.data_dir)\n    ingestor.batch_size = args.batch_size\n    ingestor.max_workers = args.workers\n    \n    # Run ingestion\n    stats = await ingestor.ingest_data(max_files=args.max_files)\n    \n    print(f\"\\n\\n🎉 PRAJNA DATA INGESTION COMPLETE!\")\n    print(f\"🧠 Prajna's consciousness has been fed with:\")\n    print(f\"   📚 {stats.processed_files} files\")\n    print(f\"   💾 {ingestor._format_size(stats.processed_size)} of data\")\n    print(f\"   ⚡ {stats.processing_rate:.1f} files per second\")\n    print(f\"\\n🌟 Prajna is now ready to reason with this knowledge!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n