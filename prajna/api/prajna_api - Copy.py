"""
Prajna API: The Core Orchestrator for TORI's Language Model with Reasoning
==========================================================================

This is the main API endpoint that coordinates all Prajna operations:
- Receives user queries from the frontend
- Orchestrates context retrieval from Soliton Memory and Concept Mesh
- Performs multi-hop cognitive reasoning when needed
- Invokes Prajna (the language model) for answer synthesis
- Runs alien overlay audit and ghost feedback analysis
- Returns structured responses with sources and trust metrics

Prajna is TORI's voice, mouth, and language model - the only component that speaks.
All language output is generated by Prajna using only ingested, traceable data.
Enhanced with cognitive reasoning capabilities for complex queries.
"""

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, AsyncGenerator
import asyncio
import logging
import time
import json
import os
import shutil
from pathlib import Path
PROJECT_ROOT = Path(__file__).resolve().parents[1]
from contextlib import asynccontextmanager

# Configure logging first
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("prajna")

# Import Prajna core components with correct paths
try:
    from ..core.prajna_mouth import PrajnaLanguageModel, generate_prajna_response
    from ..memory.context_builder import build_context, ContextResult
    from ..memory.soliton_interface import SolitonMemoryInterface
    from ..memory.concept_mesh_api import ConceptMeshAPI
    from ..audit.alien_overlay import audit_prajna_answer, ghost_feedback_analysis
    from ..config.prajna_config import PrajnaConfig
    
    # Import reasoning engine components
    from ..core.reasoning_engine import (
        PrajnaReasoningEngine, 
        detect_reasoning_triggers,
        ProductionConceptMeshAdapter,
        ReasoningMode,
        ReasoningRequest
    )
    
    # Import extraction pipeline for admin uploads
    import sys
    from pathlib import Path
    parent_dir = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(parent_dir / "ingest_pdf"))
    from pipeline import ingest_pdf_clean
    EXTRACTION_AVAILABLE = True
except ImportError as e:
    # Fallback imports for development
    logger.warning(f"Import fallback mode: {e}")
    
    # Create minimal fallback classes
    class PrajnaLanguageModel:
        def __init__(self, **kwargs): pass
        async def load_model(self): pass
        async def cleanup(self): pass
        def is_loaded(self): return True
        async def get_stats(self): return {}
        async def stream_generate(self, **kwargs): 
            yield "Fallback response in development mode"
    
    class ContextResult:
        def __init__(self):
            self.text = ""
            self.sources = []
            self.reasoning_result = None
    
    async def generate_prajna_response(**kwargs):
        class MockOutput:
            answer = "Development mode response"
        return MockOutput()
    
    async def build_context(**kwargs):
        return ContextResult()
    
    async def audit_prajna_answer(**kwargs):
        return {"trust_score": 0.8}
    
    async def ghost_feedback_analysis(**kwargs):
        return {"ghost_score": 0.7}
    
    class PrajnaConfig:
        def __init__(self):
            self.model_type = "fallback"
            self.soliton_rest_endpoint = "http://localhost:8000"
            self.soliton_ffi_enabled = False
            self.concept_mesh_in_memory = True
            self.concept_mesh_snapshot_path = ""
            self.model_path = ""
            self.device = "cpu"
            self.max_context_length = 4096
            self.temperature = 0.7
            self.start_time = time.time()
    
    class SolitonMemoryInterface:
        def __init__(self, **kwargs): pass
        async def initialize(self): pass
        async def cleanup(self): pass
        async def health_check(self): return True
        async def get_stats(self): return {}
    
    class ConceptMeshAPI:
        def __init__(self, **kwargs): pass
        async def initialize(self): pass
        async def cleanup(self): pass
        async def health_check(self): return True
        async def get_stats(self): return {}
    
    class ReasoningMode:
        EXPLANATORY = "explanatory"
        CAUSAL = "causal"
        ANALOGICAL = "analogical"
        COMPARATIVE = "comparative"
        INFERENTIAL = "inferential"
    
    class ReasoningRequest:
        def __init__(self, **kwargs): pass
    
    class PrajnaReasoningEngine:
        def __init__(self, **kwargs): pass
        async def reason(self, request): 
            class MockResult:
                confidence = 0.8
                metadata = {"reasoning_mode": "explanatory"}
                narrative_explanation = "Mock reasoning result"
                concepts_explored = 5
                reasoning_time = 1.0
                best_path = None
                paths = []
            return MockResult()
        def clear_cache(self): pass
        async def get_stats(self): return {}
    
    def ProductionConceptMeshAdapter(*args): return None
    def detect_reasoning_triggers(*args): return False
    def ingest_pdf_clean(file_path, **kwargs): 
        return {
            "status": "fallback",
            "concept_count": 0,
            "concept_names": [],
            "error_message": "Extraction pipeline not available in fallback mode"
        }
    EXTRACTION_AVAILABLE = False

# Global instances - initialized at startup
prajna_model: Optional[PrajnaLanguageModel] = None
soliton_memory: Optional[SolitonMemoryInterface] = None
concept_mesh: Optional[ConceptMeshAPI] = None
reasoning_engine: Optional[PrajnaReasoningEngine] = None
config: Optional[PrajnaConfig] = None

# Active WebSocket connections for streaming
active_connections: List[WebSocket] = []

# Admin upload directory - same as ScholarSphere for consistency
TMP_ROOT = r"{PROJECT_ROOT}\tmp"
os.makedirs(TMP_ROOT, exist_ok=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize Prajna system components at startup"""
    global prajna_model, soliton_memory, concept_mesh, reasoning_engine, config
    
    logger.info("üß† Initializing Prajna - TORI's Voice, Language Model, and Reasoning System")
    
    try:
        # Load configuration
        config = PrajnaConfig()
        logger.info(f"üìã Prajna config loaded: {config.model_type}")
        
        # Initialize Soliton Memory interface
        soliton_memory = SolitonMemoryInterface(
            rest_endpoint=config.soliton_rest_endpoint,
            ffi_enabled=config.soliton_ffi_enabled
        )
        await soliton_memory.initialize()
        logger.info("üíæ Soliton Memory interface initialized")
        
        # Initialize Concept Mesh API
        concept_mesh = ConceptMeshAPI(
            in_memory_graph=config.concept_mesh_in_memory,
            snapshot_path=config.concept_mesh_snapshot_path
        )
        await concept_mesh.initialize()
        logger.info("üï∏Ô∏è Concept Mesh API initialized")
        
        # Initialize Reasoning Engine
        if getattr(config, 'enable_reasoning', True):
            mesh_adapter = ProductionConceptMeshAdapter(concept_mesh, soliton_memory)
            reasoning_engine = PrajnaReasoningEngine(mesh_adapter)
            logger.info("üß† Prajna Reasoning Engine initialized")
        else:
            logger.info("‚è≠Ô∏è Reasoning Engine disabled")
        
        # Initialize Prajna Language Model (the mouth/voice)
        prajna_model = PrajnaLanguageModel(
            model_type=config.model_type,
            model_path=config.model_path,
            device=config.device,
            max_context_length=config.max_context_length,
            temperature=config.temperature
        )
        await prajna_model.load_model()
        logger.info(f"üó£Ô∏è Prajna Language Model loaded: {config.model_type}")
        
        yield
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Prajna: {e}")
        # Don't raise in fallback mode
        if "fallback" not in str(config.model_type if config else ""):
            raise
    finally:
        # Cleanup
        if prajna_model:
            await prajna_model.cleanup()
        if soliton_memory:
            await soliton_memory.cleanup()
        if concept_mesh:
            await concept_mesh.cleanup()
        if reasoning_engine:
            reasoning_engine.clear_cache()
        logger.info("üîÑ Prajna shutdown complete")

# Create FastAPI app with Prajna lifecycle
app = FastAPI(
    title="Prajna API - TORI's Cognitive Language Model",
    description="The voice, mouth, language model, and reasoning system of TORI",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware for frontend integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Models
class PrajnaRequest(BaseModel):
    user_query: str = Field(..., description="The user's question or prompt")
    focus_concept: Optional[str] = Field(None, description="Optional concept to focus context retrieval")
    conversation_id: Optional[str] = Field(None, description="Session/conversation identifier")
    streaming: bool = Field(False, description="Enable streaming response")
    enable_reasoning: bool = Field(True, description="Enable cognitive reasoning for complex queries")
    reasoning_mode: Optional[str] = Field(None, description="Force specific reasoning mode")

class PrajnaResponse(BaseModel):
    answer: str = Field(..., description="Prajna's generated answer")
    sources: List[str] = Field(..., description="Source references used")
    audit: Dict[str, Any] = Field(..., description="Alien overlay audit results")
    ghost_overlays: Dict[str, Any] = Field(..., description="Ghost feedback analysis")
    context_used: str = Field(..., description="Raw context provided to Prajna")
    reasoning_triggered: bool = Field(..., description="Whether reasoning was triggered")
    reasoning_data: Optional[Dict[str, Any]] = Field(None, description="Reasoning path and analysis")
    processing_time: float = Field(..., description="Time taken to generate response")
    trust_score: float = Field(..., description="Overall trust score (0.0-1.0)")

class UploadResponse(BaseModel):
    success: bool = Field(..., description="Upload success status")
    file_path: str = Field(..., description="Path to uploaded file")
    filename: str = Field(..., description="Sanitized filename")
    size_mb: float = Field(..., description="File size in MB")
    message: str = Field(..., description="Upload status message")
    extraction_performed: bool = Field(..., description="Whether concept extraction was performed")
    concept_count: int = Field(..., description="Number of concepts extracted")
    concept_names: List[str] = Field(..., description="Names of extracted concepts")
    extraction_status: str = Field(..., description="Status of extraction process")

@app.post("/api/answer", response_model=PrajnaResponse)
async def prajna_answer_endpoint(request: PrajnaRequest):
    """
    Main Prajna endpoint - generates answers using cognitive reasoning and TORI knowledge
    
    This is where Prajna (TORI's voice) speaks with full reasoning capabilities.
    All language output comes from here, enhanced with multi-hop reasoning.
    """
    start_time = time.time()
    
    try:
        logger.info(f"ü§î Prajna received query: {request.user_query[:100]}...")
        
        # Step 1: Build enhanced context from memory systems with reasoning
        logger.info("üîç Building enhanced context from TORI's memory systems...")
        context: ContextResult = await build_context(
            user_query=request.user_query,
            focus_concept=request.focus_concept,
            conversation_id=request.conversation_id,
            soliton_memory=soliton_memory,
            concept_mesh=concept_mesh,
            enable_reasoning=request.enable_reasoning and reasoning_engine is not None
        )
        
        # Step 2: Additional reasoning if requested and not already performed
        reasoning_data = None
        reasoning_triggered = context.reasoning_result is not None
        
        if (request.enable_reasoning and reasoning_engine and 
            not reasoning_triggered and request.reasoning_mode):
            
            logger.info(f"üß† Performing explicit reasoning: {request.reasoning_mode}")
            
            try:
                # Force reasoning with specified mode
                mode_map = {
                    "explanatory": ReasoningMode.EXPLANATORY,
                    "causal": ReasoningMode.CAUSAL,
                    "analogical": ReasoningMode.ANALOGICAL,
                    "comparative": ReasoningMode.COMPARATIVE,
                    "inferential": ReasoningMode.INFERENTIAL
                }
                
                reasoning_request = ReasoningRequest(
                    query=request.user_query,
                    start_concepts=[request.focus_concept] if request.focus_concept else [],
                    target_concepts=[],
                    mode=mode_map.get(request.reasoning_mode, ReasoningMode.EXPLANATORY),
                    max_hops=5,
                    min_confidence=0.3
                )
                
                reasoning_result = await reasoning_engine.reason(reasoning_request)
                
                if reasoning_result.confidence >= 0.3:
                    reasoning_triggered = True
                    context.reasoning_result = reasoning_result
                    
                    # Enhance context with reasoning (if available)
                    try:
                        from ..core.reasoning_engine import enhance_context_with_reasoning
                        context.text = await enhance_context_with_reasoning(context.text, reasoning_result)
                    except ImportError:
                        context.text += f"\n\nReasoning: {reasoning_result.narrative_explanation}"
            except Exception as e:
                logger.warning(f"Reasoning failed: {e}")
        
        # Prepare reasoning data for response
        if context.reasoning_result:
            reasoning_data = {
                "mode": context.reasoning_result.metadata.get("reasoning_mode", "unknown"),
                "confidence": context.reasoning_result.confidence,
                "narrative": context.reasoning_result.narrative_explanation,
                "concepts_explored": context.reasoning_result.concepts_explored,
                "reasoning_time": context.reasoning_result.reasoning_time,
                "path_found": context.reasoning_result.best_path is not None
            }
            
            if context.reasoning_result.best_path:
                reasoning_data["path"] = [
                    {"name": node.name, "summary": node.content_summary} 
                    for node in context.reasoning_result.best_path.nodes
                ]
                reasoning_data["coherence_score"] = context.reasoning_result.best_path.coherence_score
        
        # Step 3: Generate Prajna's response
        logger.info("üó£Ô∏è Invoking Prajna to synthesize answer...")
        prajna_output = await generate_prajna_response(
            query=request.user_query,
            context=context,
            model=prajna_model,
            streaming=request.streaming
        )
        
        # Step 4: Audit Prajna's answer with alien overlay
        logger.info("üëΩ Running alien overlay audit on Prajna's answer...")
        audit_data = await audit_prajna_answer(
            answer=prajna_output.answer,
            context=context,
            config=config
        )
        
        # Enhanced audit for reasoning-based responses
        if reasoning_triggered and context.reasoning_result:
            # Add reasoning-specific audit metrics
            audit_data["reasoning_audit"] = {
                "reasoning_confidence": context.reasoning_result.confidence,
                "path_coherence": (context.reasoning_result.best_path.coherence_score 
                                 if context.reasoning_result.best_path else 0.0),
                "reasoning_trust": min(1.0, context.reasoning_result.confidence + 
                                     audit_data.get("trust_score", 0.0)) / 2
            }
        
        # Step 5: Ghost feedback analysis
        logger.info("üëª Analyzing ghost patterns in Prajna's response...")
        ghost_data = await ghost_feedback_analysis(
            answer=prajna_output.answer,
            context=context,
            config=config,
            original_query=request.user_query
        )
        
        # Enhanced ghost analysis for reasoning
        if reasoning_triggered and context.reasoning_result:
            # Check if reasoning answered implicit questions
            ghost_data["reasoning_ghosts"] = {
                "implicit_questions_addressed": len(context.reasoning_result.best_path.nodes) - 1 if context.reasoning_result.best_path else 0,
                "reasoning_completeness": context.reasoning_result.confidence,
                "conceptual_bridges_found": len(context.reasoning_result.paths)
            }
        
        # Step 6: Calculate processing metrics
        processing_time = time.time() - start_time
        base_trust = audit_data.get("trust_score", 0.0)
        
        # Enhanced trust calculation with reasoning
        if reasoning_triggered and context.reasoning_result:
            reasoning_trust_boost = context.reasoning_result.confidence * 0.2
            trust_score = min(1.0, base_trust + reasoning_trust_boost)
        else:
            trust_score = base_trust
        
        logger.info(f"‚úÖ Prajna response complete - Trust: {trust_score:.2f}, Reasoning: {reasoning_triggered}, Time: {processing_time:.2f}s")
        
        # Return enhanced structured response
        return PrajnaResponse(
            answer=prajna_output.answer,
            sources=context.sources,
            audit=audit_data,
            ghost_overlays=ghost_data,
            context_used=context.text,
            reasoning_triggered=reasoning_triggered,
            reasoning_data=reasoning_data,
            processing_time=processing_time,
            trust_score=trust_score
        )
        
    except Exception as e:
        logger.error(f"‚ùå Prajna error: {e}")
        raise HTTPException(status_code=500, detail=f"Prajna processing failed: {str(e)}")

@app.get("/api/health")
async def prajna_health_check():
    """Health check endpoint for Prajna system"""
    health_status = {
        "prajna_model": prajna_model.is_loaded() if prajna_model else False,
        "soliton_memory": await soliton_memory.health_check() if soliton_memory else False,
        "concept_mesh": await concept_mesh.health_check() if concept_mesh else False,
        "reasoning_engine": reasoning_engine is not None,
        "timestamp": time.time()
    }
    
    all_healthy = all(health_status.values())
    status_code = 200 if all_healthy else 503
    
    return {"status": "healthy" if all_healthy else "degraded", **health_status}

@app.post("/api/upload", response_model=UploadResponse)
async def upload_pdf(file: UploadFile = File(...)):
    """
    üöÄ ADMIN BULLETPROOF UPLOAD FOR PRAJNA WITH CONCEPT EXTRACTION
    
    Admin-only PDF upload for Prajna's education and knowledge enhancement.
    Saves file AND immediately runs concept extraction pipeline!
    Only for trusted administrators - never expose to public!
    """
    try:
        start_time = time.time()
        
        logger.info(f"üì§ [PRAJNA-UPLOAD] Admin uploading: {file.filename}")
        
        # Ensure temp directory exists
        os.makedirs(TMP_ROOT, exist_ok=True)
        
        # Sanitize filename - keep only safe characters
        safe_filename = "".join(c for c in file.filename if c.isalnum() or c in '._-')
        if not safe_filename:
            safe_filename = f"prajna_upload_{int(time.time())}.pdf"
        
        # Create unique filename to avoid collisions
        timestamp = int(time.time() * 1000)
        unique_filename = f"prajna_{timestamp}_{safe_filename}"
        dest_path = os.path.join(TMP_ROOT, unique_filename)
        
        logger.info(f"üìÅ [PRAJNA-UPLOAD] Saving to: {dest_path}")
        
        # Save the file
        with open(dest_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Get file size
        file_size = os.path.getsize(dest_path)
        file_size_mb = file_size / (1024 * 1024)
        
        upload_time = time.time() - start_time
        
        logger.info(f"‚úÖ [PRAJNA-UPLOAD] Admin upload successful!")
        logger.info(f"üìè [PRAJNA-UPLOAD] Size: {file_size_mb:.2f} MB")
        logger.info(f"‚ö° [PRAJNA-UPLOAD] Upload time: {upload_time:.2f}s")
        
        # üö® NOW RUN CONCEPT EXTRACTION PIPELINE!
        extraction_performed = False
        concept_count = 0
        concept_names = []
        extraction_status = "not_attempted"
        
        if EXTRACTION_AVAILABLE:
            try:
                logger.info(f"üß¨ [PRAJNA-UPLOAD] Starting concept extraction...")
                extraction_start = time.time()
                
                # Run the same extraction pipeline as ScholarSphere
                extraction_result = ingest_pdf_clean(dest_path, extraction_threshold=0.0)
                
                extraction_time = time.time() - extraction_start
                extraction_performed = True
                
                if extraction_result.get("status") == "success":
                    concept_count = extraction_result.get("concept_count", 0)
                    concept_names = extraction_result.get("concept_names", [])
                    extraction_status = "success"
                    
                    logger.info(f"üéâ [PRAJNA-UPLOAD] Extraction successful!")
                    logger.info(f"üìä [PRAJNA-UPLOAD] Extracted {concept_count} concepts")
                    logger.info(f"‚ö° [PRAJNA-UPLOAD] Extraction time: {extraction_time:.2f}s")
                    
                    # Log some concept names for debugging
                    if concept_names:
                        sample_concepts = concept_names[:5]
                        logger.info(f"üîç [PRAJNA-UPLOAD] Sample concepts: {sample_concepts}")
                else:
                    extraction_status = "failed"
                    error_msg = extraction_result.get("error_message", "Unknown extraction error")
                    logger.error(f"‚ùå [PRAJNA-UPLOAD] Extraction failed: {error_msg}")
                    
            except Exception as extraction_error:
                logger.error(f"‚ùå [PRAJNA-UPLOAD] Extraction exception: {extraction_error}")
                extraction_status = "error"
                
        else:
            logger.warning(f"‚ö†Ô∏è [PRAJNA-UPLOAD] Extraction pipeline not available (fallback mode)")
            extraction_status = "not_available"
        
        total_time = time.time() - start_time
        
        # Enhanced success message
        if extraction_performed and extraction_status == "success":
            message = f"Admin upload and extraction successful! {concept_count} concepts extracted from {safe_filename}"
        elif extraction_performed:
            message = f"Admin upload successful but extraction {extraction_status}: {safe_filename}"
        else:
            message = f"Admin upload successful (no extraction): {safe_filename}"
        
        logger.info(f"üèÅ [PRAJNA-UPLOAD] Complete! Total time: {total_time:.2f}s")
        
        return UploadResponse(
            success=True,
            file_path=dest_path,
            filename=safe_filename,
            size_mb=round(file_size_mb, 2),
            message=message,
            extraction_performed=extraction_performed,
            concept_count=concept_count,
            concept_names=concept_names[:50],  # Limit to first 50 concepts in response
            extraction_status=extraction_status
        )
        
    except Exception as e:
        logger.error(f"‚ùå [PRAJNA-UPLOAD] Admin upload failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Admin upload failed: {str(e)}"
        )

@app.get("/api/stats")
async def prajna_statistics():
    """Get Prajna system statistics including reasoning metrics"""
    stats = {
        "model_info": await prajna_model.get_stats() if prajna_model else {},
        "memory_stats": await soliton_memory.get_stats() if soliton_memory else {},
        "mesh_stats": await concept_mesh.get_stats() if concept_mesh else {},
        "reasoning_stats": await reasoning_engine.get_stats() if reasoning_engine else {},
        "active_connections": len(active_connections),
        "uptime": time.time() - (config.start_time if config else time.time())
    }
    return stats

# Error handlers
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return {
        "error": "Prajna endpoint not found", 
        "available_endpoints": [
            "/api/answer", "/api/upload", "/api/health", "/api/stats"
        ]
    }

@app.exception_handler(500)
async def internal_error_handler(request, exc):
    logger.error(f"Prajna internal error: {exc}")
    return {"error": "Prajna internal error", "detail": "Check Prajna logs for details"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "prajna_api:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
