# serve_embeddings_bnb_4bit.py - BitsAndBytes 4-bit quantization
import os
import time
import torch
import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
from contextlib import asynccontextmanager
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer, models

# Optimize environment
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

model = None
tokenizer = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global model, tokenizer
    
    print("‚ö° Loading Qwen3-8B with BitsAndBytes 4-bit...")
    start = time.time()
    
    try:
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-Embedding-8B", trust_remote_code=True)
        
        # Load model with 4-bit quantization
        base_model = AutoModel.from_pretrained(
            "Qwen/Qwen3-Embedding-8B",
            device_map="auto",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            trust_remote_code=True
        )
        
        # Wrap in SentenceTransformer
        model = SentenceTransformer(modules=[
            models.Transformer(
                model=base_model,
                tokenizer=tokenizer,
                max_seq_length=512
            ),
            models.Pooling(
                word_embedding_dimension=base_model.config.hidden_size,
                pooling_mode_mean_tokens=True
            )
        ])
        
        print(f"‚úÖ Model loaded in {time.time() - start:.2f}s")
        print(f"‚ö° Using 4-bit quantization - expect 1.5x speedup")
        print(f"üíæ VRAM usage: ~10GB (vs 15GB for FP16)")
        
        # Warmup
        with torch.inference_mode():
            _ = model.encode(["warmup"], show_progress_bar=False)
        
        print("üöÄ Ready! Expecting ~600ms for 10 texts")
        
    except ImportError:
        print("‚ùå bitsandbytes not installed!")
        print("Install with: pip install bitsandbytes")
        raise
    
    yield
    
    del model
    torch.cuda.empty_cache()

app = FastAPI(title="TORI BnB 4-bit Service", lifespan=lifespan)

class EmbedRequest(BaseModel):
    texts: List[str]
    instruction: str = ""

@app.post("/embed")
async def embed(request: EmbedRequest):
    start = time.time()
    
    # Optional instruction formatting
    texts = request.texts
    if request.instruction:
        texts = [f"Instruct: {request.instruction}\nQuery: {text}" for text in texts]
    
    with torch.inference_mode():
        embeddings = model.encode(
            texts,
            batch_size=min(16, len(texts)),  # Smaller batch for 4-bit
            convert_to_numpy=True,
            show_progress_bar=False,
            normalize_embeddings=True
        )
    
    ms = (time.time() - start) * 1000
    
    print(f"‚ö° {len(texts)} texts in {ms:.1f}ms ({ms/len(texts):.1f}ms per text)")
    
    return {
        "embeddings": embeddings.tolist(),
        "processing_time_ms": ms,
        "texts_processed": len(texts),
        "cache_hits": 0,
        "cache_misses": len(texts)
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "model": "Qwen3-8B",
        "quantization": "BitsAndBytes 4-bit",
        "device": "cuda"
    }

if __name__ == "__main__":
    import importlib, types
    try:
        av = importlib.import_module("av")
        if not hasattr(av, "logging"):
            av.logging = types.SimpleNamespace(
                ERROR=0, WARNING=1, INFO=2, DEBUG=3,
                set_level=lambda *_, **__: None,
            )
    except:
        pass
    
    uvicorn.run(app, host="0.0.0.0", port=8080)
