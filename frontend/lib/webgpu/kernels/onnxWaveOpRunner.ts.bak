/**
 * onnxWaveOpRunner.ts
 * PLATINUM Edition: ONNX Runtime integration with zero-copy GPU tensor binding
 * 
 * Features:
 * - LRU session caching with ref-counting
 * - Zero-copy GPU tensor binding (reuses WebGPU buffers)
 * - Persistent IO bindings across frames
 * - Automatic backend selection (WebGPU â†’ WASM fallback)
 * - Session warmup and optimization
 * - Memory-efficient batch processing
 */

import { ort, InferenceSession, Tensor } from '../../ml/ort';
const { env } = ort;

export interface OnnxConfig {
    modelPath: string | Uint8Array;
    width: number;
    height: number;
    batchSize?: number;
    backend?: 'webgpu' | 'wasm' | 'auto';
    enableProfiling?: boolean;
    cacheSize?: number;  // Max cached sessions
    warmupRuns?: number;  // Runs before timing
}

interface CachedSession {
    session: InferenceSession;
    refCount: number;
    lastAccess: number;
    modelPath: string | Uint8Array;
    // ioBinding removed - not available in web
    warmupComplete: boolean;
}

export interface PerformanceMetrics {
    inferenceTime: number;
    memoryUsage: number;
    cacheHits: number;
    cacheMisses: number;
    tensorTransfers: number;
}

/**
 * Global session cache with LRU eviction
 */
class SessionCache {
    private cache: Map<string, CachedSession> = new Map();
    private maxSize: number;
    private metrics: {
        hits: number;
        misses: number;
        evictions: number;
    } = { hits: 0, misses: 0, evictions: 0 };
    
    constructor(maxSize: number = 10) {
        this.maxSize = maxSize;
    }
    
    async get(modelPath: string | Uint8Array, options?: ort.InferenceSession.SessionOptions): Promise<CachedSession> {
        const key = this.getCacheKey(modelPath, options);
        
        if (this.cache.has(key)) {
            const session = this.cache.get(key)!;
            session.refCount++;
            session.lastAccess = Date.now();
            this.metrics.hits++;
            
            console.log(`[ONNX Cache] Hit for ${typeof modelPath === "string" ? modelPath : "Uint8Array"}, refCount: ${session.refCount}`);
            return session;
        }
        
        this.metrics.misses++;
        
        // Evict LRU if at capacity
        if (this.cache.size >= this.maxSize) {
            this.evictLRU();
        }
        
        // Create new session
        const session = await this.createSession(modelPath, options);
        this.cache.set(key, session);
        
        console.log(`[ONNX Cache] Miss for ${typeof modelPath === "string" ? modelPath : "Uint8Array"}, cache size: ${this.cache.size}`);
        return session;
    }
    
    release(modelPath: string | Uint8Array, options?: ort.InferenceSession.SessionOptions): void {
        const key = this.getCacheKey(modelPath, options);
        const session = this.cache.get(key);
        
        if (session) {
            session.refCount--;
            console.log(`[ONNX Cache] Released ${typeof modelPath === "string" ? modelPath : "Uint8Array"}, refCount: ${session.refCount}`);
            
            // Don't immediately evict zero-ref sessions (they might be reused)
            if (session.refCount < 0) {
                console.warn(`[ONNX Cache] Negative refCount for ${modelPath}`);
                session.refCount = 0;
            }
        }
    }
    
    private getCacheKey(modelPath: string | Uint8Array, options?: ort.InferenceSession.SessionOptions): string {
        // Create unique key based on model and options
        const optionsStr = options ? JSON.stringify(options) : 'default';
        // Convert Uint8Array to a stable string key (use length as simple identifier)
        const pathKey = typeof modelPath === 'string' 
            ? modelPath 
            : `uint8array_${modelPath.length}_${modelPath[0]}_${modelPath[modelPath.length-1]}`;
        return `${pathKey}::${optionsStr}`;
    }
    
    private async createSession(
        modelPath: string | Uint8Array,
        options?: ort.InferenceSession.SessionOptions
    ): Promise<CachedSession> {
        const modelData = typeof modelPath === 'string'
            ? await fetch(modelPath).then(r => r.arrayBuffer()).then(b => new Uint8Array(b))
            : modelPath;
        const session = await ort.InferenceSession.create(modelData, options);
        
        return {
            session,
            refCount: 1,
            lastAccess: Date.now(),
            modelPath,
            warmupComplete: false,
        };
    }
    
    private evictLRU(): void {
        let lruKey: string | null = null;
        let lruTime = Infinity;
        
        // Find least recently used session with refCount = 0
        for (const [key, session] of this.cache.entries()) {
            if (session.refCount === 0 && session.lastAccess < lruTime) {
                lruKey = key;
                lruTime = session.lastAccess;
            }
        }
        
        if (lruKey) {
            const evicted = this.cache.get(lruKey)!;
            console.log(`[ONNX Cache] Evicting ${typeof evicted.modelPath === "string" ? evicted.modelPath : "Uint8Array"}`);
            
            // Clean up IO binding if exists
            if ((evicted as any).ioBinding) {
                (evicted as any).ioBinding.release();
            }
            
            this.cache.delete(lruKey);
            this.metrics.evictions++;
        } else {
            console.warn('[ONNX Cache] No evictable sessions (all have active references)');
        }
    }
    
    getMetrics(): typeof SessionCache.prototype.metrics {
        return { ...this.metrics };
    }
}

// Global cache instance
const globalSessionCache = new SessionCache();

/**
 * Main ONNX Wave Operator Runner
 */
export class OnnxWaveOpRunner {
    private config: Required<OnnxConfig>;
    private session: CachedSession | null = null;
    private device: GPUDevice | null = null;
    
    // Persistent IO binding for zero-copy
    private ioBinding: any /* IOBinding */ | null = null;
    private inputTensor: ort.Tensor | null = null;
    private outputTensor: ort.Tensor | null = null;
    
    // GPU buffers for zero-copy
    private gpuInputBuffer: GPUBuffer | null = null;
    private gpuOutputBuffer: GPUBuffer | null = null;
    
    // Performance tracking
    private metrics: PerformanceMetrics = {
        inferenceTime: 0,
        memoryUsage: 0,
        cacheHits: 0,
        cacheMisses: 0,
        tensorTransfers: 0,
    };
    
    constructor(config: OnnxConfig, device?: GPUDevice) {
        this.config = this.fillConfig(config);
        this.device = device || null;
    }
    
    private fillConfig(config: OnnxConfig): Required<OnnxConfig> {
        return {
            modelPath: config.modelPath,
            width: config.width,
            height: config.height,
            batchSize: config.batchSize || 1,
            backend: config.backend || 'auto',
            enableProfiling: config.enableProfiling || false,
            cacheSize: config.cacheSize || 10,
            warmupRuns: config.warmupRuns || 3,
        };
    }
    
    /**
     * Initialize the ONNX session and setup IO bindings
     */
    async initialize(): Promise<void> {
        console.log('[ONNX] Initializing with config:', this.config);
        
        // Configure ONNX Runtime
        const backend = await this.selectBackend();
        
        const sessionOptions: ort.InferenceSession.SessionOptions = {
            executionProviders: backend === 'webgpu' 
                ? [{
                    name: 'webgpu',
                    // // Not supported in current version
                    // // Not supported in current version,
                }]
                : ['wasm'],
            graphOptimizationLevel: 'all',
            enableCpuMemArena: true,
            enableMemPattern: true,
            interOpNumThreads: 4,
            intraOpNumThreads: 4,
            enableProfiling: this.config.enableProfiling,
        };
        
        // Get or create session from cache
        this.session = await globalSessionCache.get(this.config.modelPath, sessionOptions);
        
        // Setup persistent IO binding for zero-copy
        if (backend === 'webgpu' && this.device) {
            await this.setupZeroCopyBinding();
        } else {
            await this.setupStandardTensors();
        }
        
        // Warmup if needed
        if (!this.session.warmupComplete) {
            await this.warmup();
        }
        
        console.log('[ONNX] Initialization complete');
    }
    
    private async selectBackend(): Promise<'webgpu' | 'wasm'> {
        if (this.config.backend !== 'auto') {
            return this.config.backend;
        }
        
        // Auto-detect best backend
        try {
            // Check WebGPU availability
            if (this.device && 'gpu' in navigator) {
                // Try to create a WebGPU execution provider
                // Load model data first
                const modelData: Uint8Array = typeof this.config.modelPath === 'string'
                    ? await fetch(this.config.modelPath).then(r => r.arrayBuffer()).then(b => new Uint8Array(b))
                    : this.config.modelPath as Uint8Array;
                const testSession = await ort.InferenceSession.create(
                    modelData,
                    {
                        executionProviders: [{
                            name: 'webgpu',
                            // // Not supported in current version
                        }],
                    }
                );
                
                // If successful, use WebGPU
                console.log('[ONNX] WebGPU backend available');
                return 'webgpu';
            }
        } catch (e) {
            console.warn('[ONNX] WebGPU backend not available, falling back to WASM');
        }
        
        return 'wasm';
    }
    
    private async setupZeroCopyBinding(): Promise<void> {
        if (!this.device || !this.session) return;
        
        console.log('[ONNX] Setting up zero-copy GPU binding');
        
        const elementSize = 4;  // float32
        const elementsPerPixel = 2;  // Complex numbers (real, imag)
        const totalElements = this.config.width * this.config.height * 
                             this.config.batchSize * elementsPerPixel;
        const bufferSize = totalElements * elementSize;
        
        // Create persistent GPU buffers
        this.gpuInputBuffer = this.device.createBuffer({
            size: bufferSize,
            usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST | GPUBufferUsage.COPY_SRC,
            label: 'ONNX Input Buffer',
        });
        
        this.gpuOutputBuffer = this.device.createBuffer({
            size: bufferSize,
            usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.MAP_READ,
            label: 'ONNX Output Buffer',
        });
        
        // Create IO binding for zero-copy transfer
        if ('createIOBinding' in this.session.session) {
            this.ioBinding = await (this.session.session as any).createIOBinding();
            
            // Bind GPU buffers directly (zero-copy)
            const inputLocation = {
                type: 'gpu-buffer',
                gpuBuffer: this.gpuInputBuffer,
                dimensions: [this.config.batchSize, 2, this.config.height, this.config.width],
                dataType: 'float32',
            };
            
            const outputLocation = {
                type: 'gpu-buffer',
                gpuBuffer: this.gpuOutputBuffer,
                dimensions: [this.config.batchSize, 2, this.config.height, this.config.width],
                dataType: 'float32',
            };
            
            // Bind to named inputs/outputs
            const inputNames = this.session.session.inputNames;
            const outputNames = this.session.session.outputNames;
            
            (this.ioBinding as any).bindInput(inputNames[0], inputLocation);
            (this.ioBinding as any).bindOutput(outputNames[0], outputLocation);
            
            console.log('[ONNX] Zero-copy binding established');
        } else {
            console.warn('[ONNX] IO binding not supported, falling back to standard tensors');
            await this.setupStandardTensors();
        }
    }
    
    private async setupStandardTensors(): Promise<void> {
        const shape = [
            this.config.batchSize,
            2,  // Complex (real, imag)
            this.config.height,
            this.config.width,
        ];
        
        const totalElements = shape.reduce((a, b) => a * b, 1);
        const data = new Float32Array(totalElements);
        
        this.inputTensor = new ort.Tensor('float32', data, shape);
        this.outputTensor = null;  // Will be created by inference
        
        console.log('[ONNX] Standard tensors created');
    }
    
    private async warmup(): Promise<void> {
        if (!this.session) return;
        
        console.log(`[ONNX] Starting warmup (${this.config.warmupRuns} runs)`);
        
        for (let i = 0; i < this.config.warmupRuns; i++) {
            if (this.ioBinding) {
                // Zero-copy warmup
                await (this.session.session as any).run(null, this.ioBinding);
            } else if (this.inputTensor) {
                // Standard tensor warmup
                const feeds = { 'input': this.inputTensor };
                await this.session.session.run(feeds);
            }
        }
        
        this.session.warmupComplete = true;
        console.log('[ONNX] Warmup complete');
    }
    
    /**
     * Run inference with zero-copy GPU binding or standard tensors
     */
    async run(inputData?: Float32Array | GPUBuffer): Promise<Float32Array> {
        if (!this.session) {
            throw new Error('ONNX session not initialized');
        }
        
        const startTime = performance.now();
        
        let result: Float32Array;
        
        if (this.ioBinding && this.gpuInputBuffer && this.gpuOutputBuffer) {
            // Zero-copy path
            result = await this.runZeroCopy(inputData as GPUBuffer);
        } else {
            // Standard tensor path
            result = await this.runStandard(inputData as Float32Array);
        }
        
        this.metrics.inferenceTime = performance.now() - startTime;
        return result;
    }
    
    private async runZeroCopy(inputBuffer?: GPUBuffer): Promise<Float32Array> {
        if (!this.device || !this.ioBinding || !this.gpuOutputBuffer) {
            throw new Error('Zero-copy not properly initialized');
        }
        
        // If provided, copy input data to GPU buffer
        if (inputBuffer) {
            const encoder = this.device.createCommandEncoder();
            encoder.copyBufferToBuffer(
                inputBuffer,
                0,
                this.gpuInputBuffer!,
                0,
                inputBuffer.size
            );
            this.device.queue.submit([encoder.finish()]);
        }
        
        // Run inference with IO binding (zero-copy)
        await (this.session!.session as any).run(null, this.ioBinding);
        
        // Read back results
        const resultBuffer = this.device.createBuffer({
            size: this.gpuOutputBuffer.size,
            usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ,
        });
        
        const encoder = this.device.createCommandEncoder();
        encoder.copyBufferToBuffer(
            this.gpuOutputBuffer,
            0,
            resultBuffer,
            0,
            this.gpuOutputBuffer.size
        );
        this.device.queue.submit([encoder.finish()]);
        
        await resultBuffer.mapAsync(GPUMapMode.READ);
        const result = new Float32Array(resultBuffer.getMappedRange().slice());
        resultBuffer.unmap();
        
        return result;
    }
    
    private async runStandard(inputData?: Float32Array): Promise<Float32Array> {
        if (!this.inputTensor) {
            throw new Error('Standard tensors not initialized');
        }
        
        // Update input tensor if data provided
        if (inputData) {
            (this.inputTensor.data as Float32Array).set(inputData);
            this.metrics.tensorTransfers++;
        }
        
        // Run inference
        const feeds = { 'input': this.inputTensor };
        const results = await this.session!.session.run(feeds);
        
        // Get output
        const outputName = Object.keys(results)[0];
        const outputTensor = results[outputName];
        
        return outputTensor.data as Float32Array;
    }
    
    /**
     * Run batch inference efficiently
     */
    async runBatch(batches: Float32Array[]): Promise<Float32Array[]> {
        if (batches.length !== this.config.batchSize) {
            throw new Error(`Expected ${this.config.batchSize} batches, got ${batches.length}`);
        }
        
        // Combine batches into single tensor
        const batchSize = batches[0].length;
        const combinedData = new Float32Array(batchSize * batches.length);
        
        for (let i = 0; i < batches.length; i++) {
            combinedData.set(batches[i], i * batchSize);
        }
        
        // Run inference
        const result = await this.run(combinedData);
        
        // Split results back into batches
        const results: Float32Array[] = [];
        for (let i = 0; i < batches.length; i++) {
            const start = i * batchSize;
            const end = start + batchSize;
            results.push(result.slice(start, end));
        }
        
        return results;
    }
    
    /**
     * Get performance metrics
     */
    getMetrics(): PerformanceMetrics {
        const cacheMetrics = globalSessionCache.getMetrics();
        return {
            ...this.metrics,
            cacheHits: cacheMetrics.hits,
            cacheMisses: cacheMetrics.misses,
        };
    }
    
    /**
     * Cleanup resources
     */
    destroy(): void {
        // Release session reference
        if (this.session) {
            globalSessionCache.release(this.config.modelPath);
            this.session = null;
        }
        
        // Clean up IO binding
        if (this.ioBinding) {
            (this.ioBinding as any).release();
            this.ioBinding = null;
        }
        
        // Destroy GPU buffers
        this.gpuInputBuffer?.destroy();
        this.gpuOutputBuffer?.destroy();
        
        console.log('[ONNX] Resources cleaned up');
    }
}

/**
 * Utility to convert ONNX model for wave operations
 */
export async function prepareOnnxModel(
    modelPath: string,
    optimizations?: {
        quantize?: boolean;
        pruneWeights?: boolean;
        fuseOperations?: boolean;
    }
): Promise<string> {
    // This would use ONNX tools to optimize the model
    // For now, just return the path
    console.log('[ONNX] Model preparation requested for:', modelPath);
    
    if (optimizations?.quantize) {
        console.log('[ONNX] Quantization requested (not implemented)');
    }
    
    if (optimizations?.pruneWeights) {
        console.log('[ONNX] Weight pruning requested (not implemented)');
    }
    
    if (optimizations?.fuseOperations) {
        console.log('[ONNX] Operation fusion requested (not implemented)');
    }
    
    return modelPath;
}

/**
 * Factory function for easy instantiation
 */
export async function createOnnxWaveRunner(
    config: OnnxConfig,
    device?: GPUDevice
): Promise<OnnxWaveOpRunner> {
    const runner = new OnnxWaveOpRunner(config, device);
    await runner.initialize();
    return runner;
}